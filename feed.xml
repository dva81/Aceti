<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://dva81.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://dva81.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-16T12:28:55+00:00</updated><id>https://dva81.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Skills vs Connectors in Microsoft Copilot Studio - Making the Right Choice</title><link href="https://dva81.github.io/blog/2024/Skills-vs-Connectors/" rel="alternate" type="text/html" title="Skills vs Connectors in Microsoft Copilot Studio - Making the Right Choice"/><published>2024-07-07T00:00:00+00:00</published><updated>2024-07-07T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/Skills-vs-Connectors</id><content type="html" xml:base="https://dva81.github.io/blog/2024/Skills-vs-Connectors/"><![CDATA[<h1 id="skills-vs-connectors-in-microsoft-copilot-studio-making-the-right-choice">Skills vs Connectors in Microsoft Copilot Studio: Making the Right Choice</h1> <p>In the evolving landscape of automation and AI integration, Microsoft Copilot Studio offers two primary methods for enhancing functionality: Skills and Connectors. Understanding the differences, strengths, and best use cases for each can help organizations make informed decisions. This post explores the key aspects of Skills and Connectors, and provides guidance on selecting the right approach for your projects.</p> <h2 id="understanding-connectors">Understanding Connectors</h2> <p>Connectors in the Power Platform provide a low-code solution to integrate various services and applications. This approach is particularly useful for users who want to extend API calls without delving into extensive coding. Here are some key points about Connectors:</p> <ul> <li><strong>Preview Availability</strong>: Connectors are currently available in preview within the Power Platform.</li> <li><strong>Low-Code Approach</strong>: Designed for ease of use, enabling users to integrate services with minimal coding.</li> <li><strong>DLP Policy Impact</strong>: Data Loss Prevention policies apply, with standard limitations allowing 500 calls per minute per connector. These limits are adjustable based on the environment.</li> <li><strong>Authentication</strong>: Proper authentication needs to be verified to ensure secure connections.</li> </ul> <h2 id="exploring-skills">Exploring Skills</h2> <p>Skills offer a more flexible, pro-code approach, requiring Azure infrastructure. This method is ideal for users with coding expertise who need to implement complex customizations. Key features of Skills include:</p> <ul> <li><strong>Pro-Code Flexibility</strong>: Greater flexibility due to the ability to write and modify code.</li> <li><strong>Azure Infrastructure</strong>: Requires setup and management of Azure resources.</li> <li><strong>Extended Capabilities</strong>: Skills can extend further than connectors, especially when paired with Copilot extensions.</li> </ul> <h2 id="user-experience-in-copilot">User Experience in Copilot</h2> <p>Regardless of whether you choose Skills or Connectors, the user experience within Copilot remains largely consistent. Both methods aim to streamline the integration process and enhance functionality.</p> <h2 id="making-the-decision">Making the Decision</h2> <p>For Proof of Concept (POC) use cases, the primary strategy is to use custom connectors. This approach allows for quick integration and testing without extensive coding. However, it’s crucial to continuously evaluate the technical and functional requirements of your project. If the need for more advanced customization arises, considering Skills might be necessary.</p> <h2 id="action-plan">Action Plan</h2> <p>To effectively implement Connectors or Skills, consider the following steps:</p> <ol> <li><strong>Performance Overview</strong>: Assess performance needs and peak usage to estimate the number of API calls required.</li> <li><strong>Limit Adjustments</strong>: Adjust limits as necessary and manage these changes from an Application Lifecycle Management (ALM) perspective.</li> <li><strong>Performance Testing</strong>: Conduct thorough performance testing to ensure the chosen method meets your requirements.</li> </ol> <p>By carefully evaluating your project’s needs and understanding the capabilities of both Skills and Connectors, you can make informed decisions that align with your goals and resources.</p> <hr/> <p>For more detailed information on using Microsoft Bot Framework skills and Power Platform connectors, refer to the official <a href="https://learn.microsoft.com/">Microsoft Copilot Studio documentation</a>.</p> <hr/> <p>My focus is on structuring, automating and managing business processes using Agile and DevOps best practices. This creates team working environments where business continuity, transparency and human capital come first. Reach out to me on LinkedIn or check out my GitHub for more tips and tricks.</p> <p>The ideas and underlying essence are original and generated by a human author. The organization, grammar, and presentation may have been enhanced by the use of AI.</p>]]></content><author><name></name></author><category term="PowerPlatform"/><category term="AI"/><category term="DevOps"/><summary type="html"><![CDATA[This article provides a foundational understanding to help you navigate the choice between Skills and Connectors in Microsoft Copilot Studio.]]></summary></entry><entry><title type="html">Azure DevOps - Easily deploy a Power platform solution</title><link href="https://dva81.github.io/blog/2024/deploy-Power-platform-solutions/" rel="alternate" type="text/html" title="Azure DevOps - Easily deploy a Power platform solution"/><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/deploy-Power-platform-solutions</id><content type="html" xml:base="https://dva81.github.io/blog/2024/deploy-Power-platform-solutions/"><![CDATA[<h1 id="azure-devops--easily-deploy-a-power-platform-solution">Azure DevOps : Easily deploy a Power platform solution</h1> <p>I have been an Azure DevOps enthusiast for years. The ease of use and flexibility of the platform are super. In this article I am sharing my not-so-complex take on deploying a Power Platform Solution across environments. I will explain how to use a repo, build pipeline and release to different environments.</p> <h2 id="repos">Repos</h2> <p>There is no need to download the solution in a repo if you are not going to manipulate the source / zip file. Using the repo for the deployment settings json file is a good practice as these are configurations that are not managed in the solution. This way you can have version control over the deployment settings. The solution versions are managed in Power Platform or you can roll back from the artifact.</p> <p><img src="https://github.com/dva81/dva81.github.io/assets/65031840/9ed4499a-b617-44de-9df4-0c93aeb1bd00" alt="image"/></p> <h2 id="pipelines----build-artifact">Pipelines - build artifact</h2> <p>I see pipelines as a means to create a deployable artifact of deployable item. So for me it can be limited to the steps to collect and package the components that need to be deployed. You can either schedule this or start this manually if you are ready to deploy.</p> <p><img src="https://github.com/dva81/dva81.github.io/assets/65031840/674d2575-bf4e-4d2b-8230-dc44428dd29c" alt="pipeline actions"/></p> <p><img src="https://github.com/dva81/dva81.github.io/assets/65031840/8502521d-d641-4a56-b9ea-ff25f860f638" alt="artifact content"/></p> <p>This package contains everything you need to deploy to any environment. So even if you want to roll back the development environment you can do it with this package.</p> <h2 id="pipelines----release">Pipelines - Release</h2> <p>The release stages are set up from the build output / artifact. Everything is in the artifact, there is no need for other sources. In this example we have three environments to deploy. This can be across tenants if needed. We use the service connections for the connections.</p> <p><img src="https://github.com/dva81/dva81.github.io/assets/65031840/96a44216-f9fd-4ec3-8ab2-7828c6861566" alt="release stages"/></p> <p>As far as actions go. Always install the tool on the agent and import the solution using the deployment settings json file we configured in the repo.</p> <p><img src="https://github.com/dva81/dva81.github.io/assets/65031840/9ef2c0b1-f4e7-4589-bc81-546229476b69" alt="stage actions"/></p> <p><img src="https://github.com/dva81/dva81.github.io/assets/65031840/0f5fb92b-1154-436f-8821-8336a7ca3347" alt="deployment settings"/></p> <h1 id="thats-it-no-need-to-overcomplicate">That’s it. No need to overcomplicate.</h1> <p>Hope this helps! Feel free to reach out on LinkedIn.</p> <p>Also check out this post if you are interested in Azure DevOps! <a href="https://www.dennisvanaelst.net/blog/2023/Docs-as-Code/">Boosting Efficiency Docs-as-Code Strategies with Power Platform and Azure DevOps</a> or this https://learn.microsoft.com/en-us/shows/devops-lab/how-to-deploy-power-platform-with-azure-devops</p>]]></content><author><name></name></author><category term="PowerPlatform"/><category term="AI"/><category term="DevOps"/><summary type="html"><![CDATA[In this article I am sharing my take on deploying a Power Platform Solution across environments. How to use a repo, build pipeline(s) and release to different environments.]]></summary></entry><entry><title type="html">Document Processing with AI Builder - A Practical Guide to the document automation toolkit</title><link href="https://dva81.github.io/blog/2024/AI-builder-quick-guide/" rel="alternate" type="text/html" title="Document Processing with AI Builder - A Practical Guide to the document automation toolkit"/><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/AI-builder%20quick%20guide</id><content type="html" xml:base="https://dva81.github.io/blog/2024/AI-builder-quick-guide/"><![CDATA[<h1 id="document-processing-with-ai-builder-a-practical-guide-to-the-document-automation-toolkit">Document Processing with AI Builder: A Practical Guide to the document automation toolkit</h1> <p>In today’s fast-paced digital landscape, document automation is an essential tool for businesses looking to improve efficiency and accuracy. Microsoft’s AI Builder provides a comprehensive solution for automating document processing, integrating seamlessly with other Microsoft services like SharePoint and Teams. This blog post delves into the workings of the Document Automation Toolkit within AI Builder, detailing the steps involved in setting it up and the adjustments necessary to tailor it to specific needs.</p> <h2 id="getting-started-out-of-the-box-functionality">Getting Started: Out-of-the-Box Functionality</h2> <p>You can access the document automation toolkit in Power Automate. Learn about the toolkit on <a href="https://learn.microsoft.com/en-us/ai-builder/doc-automation">Document automation toolkit - AI Builder | Microsoft Learn</a>. The inital installation is standard and next-next-finish. You do need some knowledge of Power Platform if you want to get started with this.<br/> The two installations I did stalled on the last step but everything looked in working order after a refresh of the browser.</p> <h3 id="1-setting-up-the-solution">1. Setting Up the Solution</h3> <p>To begin, create a new solution within the Power Platform environment and import all toolkit components. The Toolkit comes as a managed solution, so this way you can make the necesary adjustments to the flow’s, apps and dataverse tables. This step is straightforward, enabling users to swiftly set up a foundation for their document automation process. <img src="https://github.com/dva81/dva81.github.io/assets/65031840/6f6d19a2-4ba8-48f5-b961-6a53b5bc43bf" alt="image"/></p> <h3 id="2-model-configuration">2. Model Configuration</h3> <p>Next, create a simple model to complete the initial configuration steps within the app. This involves defining the types of documents to be processed and the data fields to be extracted. <img src="https://github.com/dva81/dva81.github.io/assets/65031840/9eb347fd-6b54-44da-833c-067df788d422" alt="image"/></p> <h3 id="3-base-flow-testing">3. Base Flow Testing</h3> <p>Test the base flow by integrating it with an email system. This initial test ensures that the basic document processing pipeline is functioning correctly.</p> <p>Check and test the email importer flow <img src="https://github.com/dva81/dva81.github.io/assets/65031840/70e4aa74-1942-475f-8547-48d6892ab21c" alt="image"/></p> <p>Configure the Power App to use the correct model. <img src="https://github.com/dva81/dva81.github.io/assets/65031840/4d4fd77b-46be-4219-9fa6-5b3ecc7425d4" alt="image"/></p> <p>Ready to go! <img src="https://github.com/dva81/dva81.github.io/assets/65031840/6b63ee00-ae52-4057-967d-47efda211e02" alt="image"/></p> <h3 id="4-integrating-file-imports">4. Integrating File Imports</h3> <p>Adjust the flow to accommodate file imports from SharePoint or Teams. This integration allows for a seamless transition of documents from these storage solutions into the AI Builder processing pipeline. For this I duplicated the email importer flow and adjusted the steps in the flow to retrieve the files from a sharepoint location.</p> <p>It took only a few hours to get the document automation application up and running. After processing a few documents a few opportunities for improvement emerged.</p> <h2 id="customizing-features-for-enhanced-usability">Customizing Features for Enhanced Usability</h2> <h3 id="1-enhancing-the-user-interface">1. Enhancing the User Interface</h3> <p>The out-of-the-box user interface (UI) includes an interactive table, which may not be entirely user-friendly for all scenarios like high volume input or table manipulation. To improve this, custom buttons and actions were created within the app, enhancing the overall user experience (UX) and making the toolkit more intuitive. I added an add, delete and copy row button and a general save button. The copy function is great if lines have similar text in it and were not extracted properly. And the lay-out was tweaked a bit. The pdf viewer was made a bit smaller to allow for more room for the table and fields. <img src="https://github.com/dva81/dva81.github.io/assets/65031840/22280ec1-a86a-48e5-9f5a-f66a4463c785" alt="image"/></p> <h3 id="2-overcoming-table-structure-challenges">2. Overcoming Table Structure Challenges</h3> <p>In cases where a complex table structure needs to be extracted over multiple pages, the default capabilities of AI Builder might fall short. To address this, optical character recognition (OCR) and a series of regular expressions were employed. This combination allowed for the extraction of complex data structures from documents, ensuring no information was lost during processing.</p> <h3 id="3-addressing-field-restrictions">3. Addressing Field Restrictions</h3> <p>The default character length restriction in Dataverse tables is set to for example 100 characters. For our model, longer field lengths were necessary. Adjusting these settings in Dataverse was possible but initially caused errors during testing. By tweaking these settings and running multiple tests, the issues were eventually resolved, allowing for smoother data processing. <img src="https://github.com/dva81/dva81.github.io/assets/65031840/8d4fa078-950d-4176-a25f-3801a5f67ecd" alt="image"/></p> <h3 id="4-managing-bulk-deletion">4. Managing Bulk Deletion</h3> <p>Bulk deletion of documents is not supported out of the box. To manage this, a manual query was created for document deletion within Dataverse tables. This workaround ensured that unnecessary documents could be efficiently removed without manual intervention.</p> <h2 id="conclusion">Conclusion</h2> <p>Microsoft’s AI Builder and Document Automation Toolkit offers robust features for automating document processing tasks. While the out-of-the-box functionalities provide a solid starting point, specific customizations and adjustments can significantly enhance usability and performance. By addressing UI challenges, overcoming complex table extraction issues, modifying field restrictions, and implementing manual deletion queries, businesses can tailor the AI Builder to meet their unique requirements, ensuring a more efficient and streamlined document automation process.</p>]]></content><author><name></name></author><category term="PowerPlatform"/><category term="AI"/><summary type="html"><![CDATA[This blog post delves into the workings of the Document Automation Toolkit within AI Builder, detailing the steps involved in setting it up and the adjustments necessary to tailor it to specific needs.]]></summary></entry><entry><title type="html">Streamline Your Time Tracking - How to Use Outlook and Power Automate for Efficient Reporting</title><link href="https://dva81.github.io/blog/2024/Streamline-Your-Time-Tracking/" rel="alternate" type="text/html" title="Streamline Your Time Tracking - How to Use Outlook and Power Automate for Efficient Reporting"/><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/Streamline-Your-Time-Tracking</id><content type="html" xml:base="https://dva81.github.io/blog/2024/Streamline-Your-Time-Tracking/"><![CDATA[<h1 id="document-processing-with-ai-builder-a-practical-guide-to-the-document-automation-toolkit">Document Processing with AI Builder: A Practical Guide to the document automation toolkit</h1> <p>If you are an organized consultant or developer, you might recognize the screenshot from the Outlook calendar below. It’s color-coded, organized, and includes different topics, meetings, customers, and projects. When working with various customers and projects, you often need to fill in timesheets for each one in multiple systems and formats. I constantly lose track and am not good at handling multi-faceted administration across different tools and software. To simplify this, I devised a straightforward method to log all my activities in Outlook and generate simple weekly reports.</p> <p>check out my post on LinkedIn <a href="https://www.linkedin.com/pulse/streamline-your-time-tracking-how-use-outlook-power-dennis-van-aelst-4u4ne/?trackingId=Gf8WIHj%2FQG27H8lGUZN38g%3D%3D">Streamline Your Time Tracking: How to Use Outlook and Power Automate for Efficient Reporting</a>) .</p>]]></content><author><name></name></author><category term="PowerPlatform"/><category term="AI"/><summary type="html"><![CDATA[When working with various customers and projects, you often need to fill in timesheets for each one in multiple systems and formats. I devised a straightforward method to log all my activities in Outlook and generate simple weekly reports.]]></summary></entry><entry><title type="html">A Collection of Must-Reads for the IT consultant</title><link href="https://dva81.github.io/blog/2024/Reading-list/" rel="alternate" type="text/html" title="A Collection of Must-Reads for the IT consultant"/><published>2024-02-29T00:00:00+00:00</published><updated>2024-02-29T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/Reading-list</id><content type="html" xml:base="https://dva81.github.io/blog/2024/Reading-list/"><![CDATA[<p>This is my collection of Must-Reads for the IT consultant! Happy Reading</p> <h2 id="the-devops-handbook-how-to-create-world-class-agility-reliability-and-security-in-technology-organizations-by-gene-kim">The DevOPS Handbook How to Create World-Class Agility, Reliability, and Security in Technology Organizations by Gene Kim</h2> <p>https://www.goodreads.com/book/show/26083308-the-devops-handbook</p> <h2 id="the-phoenix-project-by-gene-kim">The Phoenix Project by Gene Kim</h2> <p>https://www.goodreads.com/book/show/17255186-the-phoenix-project</p> <h2 id="pmi-acp-exam-prep-rapid-learning-to-pass-the-pmi-agile-certified-practitioner-pmi-acp-exam">Pmi-acp Exam Prep: Rapid Learning to Pass the Pmi Agile Certified Practitioner Pmi-acp Exam</h2> <p>https://www.goodreads.com/book/show/27276014-pmi-acp-exam-prep</p> <h2 id="the-scrum-guide">The Scrum Guide</h2> <p>https://www.scrum.org/resources/scrum-guide</p> <h2 id="scrum-product-ownership-by-robert-galen">Scrum Product ownership by Robert Galen</h2> <p>https://www.goodreads.com/book/show/44566037-scrum-product-ownership</p> <h2 id="atomic-design">Atomic Design</h2> <p>https://atomicdesign.bradfrost.com</p> <h2 id="flawless-consulting-a-guide-to-getting-your-expertise-used-by-peter-block">Flawless Consulting: A Guide to Getting Your Expertise Used by Peter Block</h2> <p>https://www.goodreads.com/book/show/8265721-flawless-consulting</p> <h2 id="the-7-habits-of-highly-effective-people-powerful-lessons-in-personal-change-by-steven-covey">The 7 Habits of Highly Effective People: Powerful Lessons in Personal Change by Steven Covey</h2> <p>https://www.goodreads.com/book/show/36072.The_7_Habits_of_Highly_Effective_People</p> <h2 id="about-the-archimate-modeling-language">About the ArchiMate Modeling Language</h2> <p>https://www.opengroup.org/archimate-forum/archimate-overview</p> <h2 id="dealing-with-people-you-cant-stand-how-to-bring-out-the-best-in-people-at-their-worst">Dealing with People You Can’t Stand: How to Bring Out the Best in People at Their Worst</h2> <p>https://www.goodreads.com/book/show/734384.Dealing_with_People_You_Can_t_Stand</p>]]></content><author><name></name></author><category term="Agile"/><category term="Leadership"/><summary type="html"><![CDATA[This is my collection of Must-Reads for the IT consultant.]]></summary></entry><entry><title type="html">Automating LinkedIn Posts - A Simple Scheduler for Busy Professionals</title><link href="https://dva81.github.io/blog/2024/Automating-LinkedIn-Posts/" rel="alternate" type="text/html" title="Automating LinkedIn Posts - A Simple Scheduler for Busy Professionals"/><published>2024-01-11T00:00:00+00:00</published><updated>2024-01-11T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/Automating-LinkedIn-Posts</id><content type="html" xml:base="https://dva81.github.io/blog/2024/Automating-LinkedIn-Posts/"><![CDATA[<p>As a practice lead, staying active on LinkedIn is crucial for promoting both yourself and your business. Many professionals dedicate time to post regularly or leverage scheduling functions to maintain a consistent online presence. However, when it comes to bulk scheduling posts over an extended period, LinkedIn lacks a native feature. Even the LinkedIn API falls short in this aspect, leaving you in need of a customized solution.</p> <p>In my quest to efficiently manage my LinkedIn content, I decided to create a simple scheduler that allows me to plan my posts for the next two years. In this blog post, I’ll walk you through the process of building a Power Automate Flow that meets this requirement.</p> <h1 id="requirements">Requirements</h1> <ul> <li>Dynamic Message Creation: Ensure variation in posts to keep your content engaging.</li> <li>Bi-weekly Scheduling: Automate the process to post every two weeks.</li> <li>Maintain Control: Craft a solution that keeps you in the driver’s seat.</li> </ul> <h1 id="building-the-power-automate-flow">Building the Power Automate Flow</h1> <p><img src="/assets/img/Automating LinkedIn Posts/Automating LinkedIn Posts (3).png" alt="Automating LinkedIn Posts"/></p> <h2 id="step-1-connect-with-linkedin">Step 1 Connect with LinkedIn</h2> <p>The first step involves creating a connection with LinkedIn using the standard free connector. Simply use your credentials to log in and establish the connection. <img src="/assets/img/Automating LinkedIn Posts/Automating LinkedIn Posts (2).png" alt="Automating LinkedIn Posts"/></p> <h2 id="step-2-dynamic-message-creation">Step 2 Dynamic Message Creation</h2> <p>To add variety to your posts, leverage Power Automate’s functions. Utilize the rand function to dynamically generate messages, ensuring a mix of content in your posts. <img src="/assets/img/Automating LinkedIn Posts/Automating LinkedIn Posts (5).png" alt="Automating LinkedIn Posts"/></p> <p><img src="/assets/img/Automating LinkedIn Posts/Automating LinkedIn Posts (1).png" alt="Automating LinkedIn Posts"/></p> <h2 id="step-3-bi-weekly-scheduling">Step 3 Bi-weekly Scheduling</h2> <p>Implement a scheduling mechanism within the flow to post every two weeks. This ensures a consistent presence on your LinkedIn profile. <img src="/assets/img/Automating LinkedIn Posts/Automating LinkedIn Posts (4).png" alt="Automating LinkedIn Posts"/></p> <p>Step 4 Stay in Control The entire solution is designed to keep you in control. By dynamically creating messages, scheduling bi-weekly posts, and leveraging Power Automate’s capabilities, you can confidently manage your LinkedIn content for the next two years.</p> <h1 id="access-the-power-automate-flow">Access the Power Automate Flow</h1> <p>To implement this solution, you can download the source files directly from <a href="https://github.com/dva81/PowerAutomate">GitHub</a>. Don’t forget to turn the flow on after setting it up.</p> <p>Empower your LinkedIn strategy by taking charge of your posting schedule with this simple yet effective Power Automate Flow. Stay visible, stay engaged, and let automation work for you!</p>]]></content><author><name></name></author><category term="PowerPlatform"/><summary type="html"><![CDATA[Many professionals dedicate time to post regularly or leverage scheduling functions to maintain a consistent online presence. However, when it comes to bulk scheduling posts, LinkedIn lacks a native feature, leaving you in need of a customized solution.]]></summary></entry><entry><title type="html">Mastering AI Model Validation - A Comprehensive Guide for Success</title><link href="https://dva81.github.io/blog/2024/Model-Validation/" rel="alternate" type="text/html" title="Mastering AI Model Validation - A Comprehensive Guide for Success"/><published>2024-01-02T00:00:00+00:00</published><updated>2024-01-02T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/Model-Validation</id><content type="html" xml:base="https://dva81.github.io/blog/2024/Model-Validation/"><![CDATA[<p>AI is everywhere these days! Yet is has existed since the beginning of the computer age. What is so different from 25 years ago? What makes it more powerful, more present? Why all the fuss? Over the last 10 years machine learning has become extremely powerful.</p> <p>Rather than programmers giving machine learning AIs a definitive list of instructions on how to complete a task, the AIs have to learn how to do the task themselves. There are many ways to attempt this, but the most popular approach involves software that is trained by example.</p> <h1 id="the-importance-of-validating-ai-models">The importance of validating AI models</h1> <p>AI is already involved in making important processes (like translations and summarizations) and yet AI is often biased, meaning its recommendations can be too. Time after time, researchers have found that <a href="https://www.newscientist.com/article/2166207-discriminating-algorithms-5-times-ai-showed-prejudice/">neural networks pick up biases from the data sets they are trained on</a>. For example, face recognition algorithms have much lower accuracy for anyone who is not a white man.</p> <p>AI decisions are also opaque or referred to as a “Black box”. How neural networks come to conclusions is very hard to analyze, meaning if they make a crucial mistake, such as missing cancer in an image, it’s very difficult to find out why they made the mistake or to hold them accountable.</p> <blockquote> <p>Validating AI models ensures accuracy, robustness, and reliability.</p> </blockquote> <p>Often creating and validating these sets in a time and resource consuming activity which does not always help the business case due to its labor-intensive nature. But validating AI models is necessary and ensures accuracy, robustness, and reliability.</p> <h1 id="data-splitting-for-model-validation">Data Splitting for Model Validation</h1> <p>The primary reason for data splitting is to avoid <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> , which occurs when the model is too complex and fits the training data too well, resulting in poor generalization performance on new data. <a href="https://machinelearningmastery.com/training-validation-test-split-and-cross-validation-done-right/">By splitting the data, we can ensure that the model is not overfitting to the training data and is generalizing well to new data</a>.</p> <p>Data splitting also provides an unbiased estimate of the model’s performance. If we use the same data for training and testing, the model will perform well on the training data but may not generalize well to new data. By using a separate testing set, we can obtain an unbiased estimate of the model’s performance on new data.</p> <h1 id="determining-validation-set-size">Determining Validation Set Size</h1> <p>To calculate the size of the validation set, there is no one-size-fits-all answer. However, a common approach is to use a 70/30 or 80/20 split for the training and validation sets, respectively. The size of the validation set should be large enough to provide a representative sample of the data but small enough to avoid overfitting. A good rule of thumb is to use at least 5% of the total data for the validation set. That means that for a population of 40000 you need a sample set of 2000.</p> <p>This online calculator helps to calculate sample sizes - <a href="https://www.calculator.net/sample-size-calculator.html">Sample Size Calculator</a> - for test and validation purposes. It provides insights into how the right validation set size contributes to an effective model assessment.</p> <p><img src="/assets/img/Model-validation/model-validation (4).png" alt="online calculator"/></p> <p>Methods of Model Validation: Cross Validation Cross-validation is a technique used in machine learning to evaluate the performance of a model. It involves dividing the dataset into smaller groups and then averaging the performance in each group to reduce the impact of partition randomness on the results. Nevertheless, each method has its own bias toward more positive or negative results. Here are some of the most commonly used cross-validation techniques:</p> <ol> <li> <p>Train-Test Split Method: This method randomly partitions the dataset into two subsets (called training and test sets) so that the predefined percentage of the entire dataset is in the training set. Then, we train our machine learning model on the training set and evaluate its performance on the test set. In this way, we are always sure that the samples used for training are not used for evaluation and vice versa.</p> </li> <li> <p>K-Fold Cross-Validation: In k-fold cross-validation, we first divide our dataset into k equally sized subsets. Then, we repeat the train-test method k times such that each time one of the k subsets is used as a test set and the rest k-1 subsets are used together as a training set. Finally, we compute the estimate of the model’s performance estimate by averaging the scores over the k trials.</p> </li> <li> <p>Leave-One-Out Cross-Validation: In leave-one-out cross-validation, we use all but one sample for training and the remaining sample for testing. We repeat this process for all samples in the dataset and average the performance over all trials.</p> </li> </ol> <h1 id="a-power-platform-example">A Power Platform Example</h1> <p>Let us say we have created a model in <a href="https://learn.microsoft.com/en-us/ai-builder/">AI builder</a>, and you want to see if it works as expected. Power Platform comes some standard prediction engines and features, still it is always a good idea to validate the outcome of a process.</p> <p>This technique can also be used with models created by other technologies as well.</p> <p>An easy way to do this in Power platform is with a Power Automate flow.</p> <ol> <li>Put your validation files in one folder. (you can even create them using a generation flow)</li> </ol> <p><img src="/assets/img/Model-validation/model-validation (3).png" alt="generation flow"/></p> <ol> <li> <p>Create a database with all the (meta)data you expect the model to find in the files.</p> </li> <li> <p>Create a flow that reads the files, runs the model and verifies the data found against the data source.</p> </li> <li> <p>Put the results in a file or dashboard.</p> </li> </ol> <p>It can look something like this:</p> <p><img src="/assets/img/Model-validation/model-validation (2).png" alt="flow"/></p> <h1 id="key-takeaways">Key Takeaways</h1> <p>Data splitting is essential for avoiding overfitting and obtaining an unbiased estimate of the model’s performance. It is a crucial step in the validation process. The choice of cross-validation technique depends on the size of the dataset, the number of features, and the computational resources available.</p> <p>These techniques are used to evaluate the performance of a machine learning model and to reduce the impact of partition randomness on the results. It helps ensure the overall accuracy, robustness, and reliability of the business process.</p> <p>Check out <a href="https://machinelearningmastery.com/">Machine Learning Mastery</a> where you’ll find “not so dry” material on this topic.</p>]]></content><author><name></name></author><category term="PowerPlatform"/><category term="AI"/><summary type="html"><![CDATA[AI is everywhere these days! Yet is has existed since the beginning of the computer age. What is so different from 25 years ago? What makes it more powerful, more present? Why all the fuss? Over the last 10 years machine learning has become extremely powerful.]]></summary></entry><entry><title type="html">Boosting Efficiency Docs-as-Code Strategies with Power Platform and Azure DevOps</title><link href="https://dva81.github.io/blog/2023/Docs-as-Code/" rel="alternate" type="text/html" title="Boosting Efficiency Docs-as-Code Strategies with Power Platform and Azure DevOps"/><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2023/Docs-as-Code</id><content type="html" xml:base="https://dva81.github.io/blog/2023/Docs-as-Code/"><![CDATA[<p>Some time ago I took over a Power Platform project right after first deploys to production. The application is a collection of over 30 Power Apps and more than 600 other components. With my DevOps background I was really happy to see the Azure DevOps setup the team had put in place to automate the deploy of the hundreds of Power Platform components as much a as possible.</p> <p>If you want to see how that can be done check out this video by Feline Parein. <a href="https://www.youtube.com/watch?v=pv8CyKrL5ds">Feline Parein - How to deploy Power Platform solution company-wide: Azure DevOps can help with CI/CD - YouTube</a></p> <p>As I was new to the project, I had no context to inform my understanding of what was going on or how things were connected. So, I started reading and looking for information. The developers were working on all the new features and releases, so they had little time to address my information needs.</p> <h1 id="why-docs-as-code">Why docs-as-code?</h1> <p>I found a maze of documents, snippets, designs and much more without any solid structure. Everything was there but it was in chaos. After talking to the team, I found out that, like most developers they don’t like to write documentation, switching tools and work environments, there was no review process, and they did not see the purpose of it as “everything we build is Low-Code”.</p> <blockquote> <p>My next mission was clear: improve the process, improve the documentation.</p> </blockquote> <p>With docs-as-code, you treat your documentation in the same way as your code. A quick start:</p> <ul> <li> <p>Format: pick a format that is easily versioned and transformed like .md. (Markdown)</p> </li> <li> <p>Versioning: Use a versioning system like GIT just as you would with code or configuration files. No need to append a date or v2.2x at the end.</p> </li> <li> <p>Automation: Transform the docs to any format needed (html, pdf, …) and distribute them in one go.</p> </li> <li> <p>Validation: Use an approval flow to get the documents approved and signed off.</p> </li> </ul> <h1 id="how-did-we-do-this">How did we do this?</h1> <p>The good thing was the team already tried to put documentation in repositories. We built on those first steps and completed the process.</p> <ul> <li>Create a wiki and publish it as code</li> </ul> <p><img src="/assets/img/Docs-as-code/Docs-as-Code1.png" alt="publish it as code"/></p> <ul> <li>.md as format. Markdown is a lightweight markup language that you can use to add formatting elements to plaintext text documents.</li> </ul> <p><img src="/assets/img/Docs-as-code/Docs-as-Code2.png" alt="publish it as code"/></p> <p><img src="/assets/img/Docs-as-code/Docs-as-Code3.png" alt="publish it as code"/></p> <ul> <li>In the pipeline step the output is generated in pdf format using a standard task in Azure DevOps. and attached to the artifact for deployment.</li> </ul> <p><img src="/assets/img/Docs-as-code/Docs-as-Code4.png" alt="publish it as code"/></p> <ul> <li>Releasing the documentation. In this step we automated the release to the right location for validation. After approval in Azure Devops that version was copied to the end user or production locations in the Power Platform solution.</li> </ul> <p><img src="/assets/img/Docs-as-code/Docs-as-Code6.png" alt="publish it as code"/></p> <p><img src="/assets/img/Docs-as-code/Docs-as-Code5.png" alt="publish it as code"/></p> <h1 id="bringing-it-all-together">Bringing it all together</h1> <blockquote> <p>“Happy cows make better milk.”</p> </blockquote> <p>By improving the overall documentation process, the team is now much more inclined to write documentation and maintain it. Connecting other people to the project is now much easier and we created better continuity for our client as well.</p> <hr/> <p>If you are more interested in the view of a technical writer, you might want to start your journey at http://writethedocs.org.</p> <hr/> <p>My focus is on structuring, automating and managing business processes using Agile and DevOps best practices. This creates team working environments where business continuity, transparency and human capital come first. Reach out to me on LinkedIn or check out my GitHub for more tips and tricks.</p>]]></content><author><name></name></author><category term="DevOps"/><category term="PowerPlatform"/><summary type="html"><![CDATA[Some time ago I took over a Power Platform project right after first deploys to production. The application is a collection of over 30 Power Apps and more than 600 other components. With my DevOps background I was really happy to see the Azure DevOps setup the team had put in place to automate the deploy of the hundreds of Power Platform components as much a as possible.]]></summary></entry><entry><title type="html">Agile product owner - key responsibilities in scrum development teams</title><link href="https://dva81.github.io/blog/2023/Agile-product-owner/" rel="alternate" type="text/html" title="Agile product owner - key responsibilities in scrum development teams"/><published>2023-11-12T00:00:00+00:00</published><updated>2023-11-12T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2023/Agile-product-owner</id><content type="html" xml:base="https://dva81.github.io/blog/2023/Agile-product-owner/"><![CDATA[<p>Working at ACOLAD as an ECM Team Coach, most of my time is spent leading product teams in implementing common-of-the-shelf software (COTS) applications in enterprise environments. This includes introducing agile software development methodologies and connecting business users and developers in one t-shaped, multifunctional team.</p> <p>When looking at well performing scrum teams, one thing is clear: the product owner (PO) is the key to a successful product while the team is the locked treasure chest.</p> <h1 id="key-responsibilities-of-the-agile-product-owner">Key responsibilities of the Agile product owner</h1> <p>The PO plays one of the three roles (scrum master, dev team and product owner) of the scrum framework for agile project management. He s the only person responsible for managing the Product Backlog, put simply, the list of all things that needs to be done within the project. These can be either technical or user-centric, e.g. in the form of user stories (US). Clearly expressing product backlog items and translating stakeholder needs into usable instructions for creating and delivering solutions to those challenges. This is key to successfully starting any agile software development project.</p> <p>In many cases, business stakeholders on the client side are not ready to level with the tempo of the agile team and will not deliver information and requirements in a way the development team easily understands on what needs to be done. To connect the two, the PO must ensure that the Product Backlog is visible, transparent, clear to all and shows what the team will work on next to complete the goal or deadline!</p> <p>So how can an agile product owner gain trust from his team and value for the client in any software development project? As per my experience, I have listed the top 4 areas in which a Product Owner must perform well to keep scrum teams effective:</p> <h2 id="you-are-a-leader">You are a leader!</h2> <p>You lead the product development… getting the most value for the client. Agile dictates a self-organizing team and the product owner is second servant-leader in the team, next to the Agile lead or Scrum master. Always deliver added business value even if the development team wants to clean up technical debt. Get grip on your resources and lead them to a better product, but also be aware that code/infra is a living organism.</p> <h2 id="product-purpose---mission">Product Purpose - Mission</h2> <p>Know your product/mission in and out. Breathe it, live it. Envision it.</p> <p>Focus more on the team instead of business stakeholders. Stakeholders are important but ensure you don t lose the grip on the dev team!</p> <p>Take ownership of the project and demos, don t rely on the dev team to show what was accomplished as a team. Get involved in all the processes from deployment to production and also in maintenance operations, gathering necessary feedback to continuously improve.</p> <h2 id="know-your-backlog">Know your backlog</h2> <p>The team can only start working if they clearly know what to do. Backlog management must be done properly. The PO must challenge the development team to improve the backlog, connect with the target audience (users) and log progress in an issue tracker. </p> <p>What is important? Prioritize, not everything is priority 1.</p> <p>Incident control is not the same as the introduction of a new feature or US and vice versa, so they cannot be simply pulled in a sprint. Get your stories ready by making sure User Acceptance Tests (UAT) and operations acceptance criteria are clear.</p> <h2 id="delivering-quality">Delivering quality</h2> <p>Inspect and adapt! This is something we hear often in Agile. The PO is responsible for the value that will be delivered by the whole team, but value also means quality. There is no value if the users don t want to accept it and the product is not up to standards.</p> <p>The PO should check or inspect all aspects of Definition of Done. Are they realistic? Make sure that the project s priorities are well understood by the software development teams. In many cases the software development team only delivers what they think is important. Sure, communication is there, but people tend to work on what they like best and not on what should be done. The PO must keep a close eye on what will be delivered.</p> <h1 id="conclusion">Conclusion</h1> <p>The Agile product owner is the second servant leader in the scrum team, with focus on the word leader . Whether you re in an agile methodology or not, you know the role of the Product Owner is instrumental for the success of any software development project. In more unexperienced teams, where project management is evolving towards more agile practices and an agile mindset, the PO can make an even bigger difference in carrying the mission to deliver a high-quality solution.</p> <p>Updated 12/01/2022</p>]]></content><author><name></name></author><category term="Agile"/><category term="Leadership"/><summary type="html"><![CDATA[Working at ACOLAD as an ECM Team Coach, most of my time is spent leading product teams in implementing common-of-the-shelf software (COTS) applications in enterprise environments. This includes introducing agile software development methodologies and connecting business users and developers in one t-shaped, multifunctional team.]]></summary></entry><entry><title type="html">Content Capture the human factor - exploring user adoption</title><link href="https://dva81.github.io/blog/2023/Content-Capture/" rel="alternate" type="text/html" title="Content Capture the human factor - exploring user adoption"/><published>2023-10-11T00:00:00+00:00</published><updated>2023-10-11T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2023/Content-Capture</id><content type="html" xml:base="https://dva81.github.io/blog/2023/Content-Capture/"><![CDATA[<p>Advances in data capture and document recognition technologies are leading more and more organizations to integrate them into their content management solutions. Embedded with advanced artificial intelligence (AI) capabilities, these systems scan all incoming communications, import documents, recognize and classify data, images or videos. But it doesn’t end there. After the content type is identified, classified and data captured in a digital format, this information is routed within the business environment.</p> <p>While all this is happening, the AI engine enriches the meta-data by reading the content and making decisions as if they were done by a human.</p> <p>Enterprise capture management empowers all employees to access the right content at the right time, by streamlining the lifecycle of their information. This means that from data capture to distribution and archiving, your organization is able to virtually eliminate paper-based information and enhance content visibility. Not forgetting to combine the necessary content compliance and information security functionalities.</p> <p>Surprisingly, the lack of effective adoption is the single largest factor impacting the ROI of implementing enterprise capture technologies. From a technical perspective, most requirements are feasible. Robotics, automation tooling and content intelligence are becoming more and more accessible, requiring less and less in-depth expertise on each technology. However, reluctant users who avoid bringing the new technology into their day-to-day tasks means you re not making the most of your technology investment.</p> <h1 id="the-resistance">The resistance</h1> <p>User adoption is oddly related to resistance to change.</p> <p>Although the premise of user adoption is changing from an outdated or redundant system to a newer, more efficient one, getting everyone on board isn’t as easy as you would expect.</p> <p>Here are a few tips I’ve picked up over the years that can help you get through these challenges.</p> <ol> <li>Document your content management processes</li> </ol> <p>Take implementing content management process automation powered by artificial intelligence as an example. One of the biggest challenges is getting users who know every step of the (manual) process to put it in words.  Generally this isn’t documented and it s the key to identify what steps can be automated and where AI can help make their job easier.</p> <p>To prevent this, make sure your organization is documenting processes by using models and diagrams but keep them simple and understandable so that those with no knowledge of business process management (BPM) can understand the workflows and suggest improvements.</p> <ol> <li>Follow the KISS principle</li> </ol> <p>Implementing content capture technologies can seem daunting for organizations with large amounts of critical information based on paper and no in-house technical background or know-how.</p> <p>It also takes a lot of effort, starting from the teams involved in gathering requirements, to the consultants and AI engineers implementing the solution. All AI is data-driven, so configuring these types of knowledge management solutions is labor intensive due to the sheer amount of data samples it demands.</p> <p>By promoting an agile iterative <a href="https://blog.amplexor.com/taking-fast-track-content-management-projects">implementation approach</a> we start with a very small reference set, work on that, and then move to production as soon as possible. Receiving input from all the teams involved in the project and keeping them up-to-date on progress and milestones is key. Users need to feel encouraged from the first iteration and throughout the development and implementation stages keeping it simple and straightforward helps.</p> <ol> <li>Make sure your teams are involvedHaving worked as an ECM Consultant for organizations of all sizes for nearly ten years, I can say that most content capture solution implementations are technology driven. There s usually no clear change management strategy to ensure smooth adoption on the end-user level.</li> </ol> <p>Sure, the company probably has a business case and a business sponsor, but when it comes to communicating to users what s changing and how and getting them excited about the project, it s normally a reactive event. Very often we see user adoption initiatives being limited to assigning a team member to handle User Acceptance Testing (UAT), after which the system is implemented, and an educational video is expected to do the job.</p> <p>End-users of any technology you re looking to implement should be involved from the early stages of the project.</p> <ol> <li>Use a timely, frequent and transparent communication</li> </ol> <p>What is the purpose of process automation and expected improvements in day-to-day work? Have specific timelines been set or will the implementation be completed in phases?</p> <p>Sometimes the business case is just not clear. And while you don t need to share every detail about the project, communicating key information with your teams early and often will keep them in the loop and allow them to know what to expect.</p> <p>This happened on a project I worked on. After requirements intake and building the content capture solution, we moved to UAT. An end-user started working with the application and all its functionalities: automated data extraction and classification. Basically text boxes and fields are automatically populated so less manual input is needed from the user. Suddenly, she said: If this is automated, what am I going to do all day?  She had no clue what was going on because the organization didn t focus on change management.</p> <p>&lt; “Success consists of going from failure to failure without loss of enthusiasm.”</p> <ul> <li>Winston Churchill</li> </ul> <p>Focus on your strategy’s goal, make it clear and refer to it often. Content capture, artificial intelligence, natural language processing and machine learning are all intimidating subjects in the workplace with a lot of associated misconceptions. Remember that the human factor drives the success of any technology deployment - after all, success is measured by the extent to which people embrace and incorporate new practices and behaviors into their daily work routines, how they adopt and adapt the technology for their own purpose.</p> <p>First published 14/01/2022 - https://blog.acolad.com/</p>]]></content><author><name></name></author><category term="Capture"/><category term="Capture"/><summary type="html"><![CDATA[Advances in data capture and document recognition technologies are leading more and more organizations to integrate them into their content management solutions. Embedded with advanced artificial intelligence (AI) capabilities, these systems scan all incoming communications, import documents, recognize and classify data, images or videos. But it doesn't end there. After the content type is identified, classified and data captured in a digital format, this information is routed within the business environment.]]></summary></entry></feed>