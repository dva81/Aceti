<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://dva81.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://dva81.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-20T17:12:39+00:00</updated><id>https://dva81.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Automating LinkedIn Posts - A Simple Scheduler for Busy Professionals</title><link href="https://dva81.github.io/blog/2024/Automating-LinkedIn-Posts/" rel="alternate" type="text/html" title="Automating LinkedIn Posts - A Simple Scheduler for Busy Professionals"/><published>2024-01-11T00:00:00+00:00</published><updated>2024-01-11T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/Automating-LinkedIn-Posts</id><content type="html" xml:base="https://dva81.github.io/blog/2024/Automating-LinkedIn-Posts/"><![CDATA[<p>As a practice lead, staying active on LinkedIn is crucial for promoting both yourself and your business. Many professionals dedicate time to post regularly or leverage scheduling functions to maintain a consistent online presence. However, when it comes to bulk scheduling posts over an extended period, LinkedIn lacks a native feature. Even the LinkedIn API falls short in this aspect, leaving you in need of a customized solution.</p> <p>In my quest to efficiently manage my LinkedIn content, I decided to create a simple scheduler that allows me to plan my posts for the next two years. In this blog post, I’ll walk you through the process of building a Power Automate Flow that meets this requirement.</p> <h1 id="requirements">Requirements</h1> <ul> <li>Dynamic Message Creation: Ensure variation in posts to keep your content engaging.</li> <li>Bi-weekly Scheduling: Automate the process to post every two weeks.</li> <li>Maintain Control: Craft a solution that keeps you in the driver’s seat.</li> </ul> <h1 id="building-the-power-automate-flow">Building the Power Automate Flow</h1> <p><img src="/assets/img/Automating LinkedIn Posts/Automating LinkedIn Posts (3).png" alt="Automating LinkedIn Posts"/></p> <h2 id="step-1-connect-with-linkedin">Step 1 Connect with LinkedIn</h2> <p>The first step involves creating a connection with LinkedIn using the standard free connector. Simply use your credentials to log in and establish the connection. <img src="/assets/img/Automating LinkedIn Posts/Automating LinkedIn Posts (2).png" alt="Automating LinkedIn Posts"/></p> <h2 id="step-2-dynamic-message-creation">Step 2 Dynamic Message Creation</h2> <p>To add variety to your posts, leverage Power Automate’s functions. Utilize the rand function to dynamically generate messages, ensuring a mix of content in your posts. <img src="/assets/img/Automating LinkedIn Posts/Automating LinkedIn Posts (5).png" alt="Automating LinkedIn Posts"/></p> <p><img src="/assets/img/Automating LinkedIn Posts/Automating LinkedIn Posts (1).png" alt="Automating LinkedIn Posts"/></p> <h2 id="step-3-bi-weekly-scheduling">Step 3 Bi-weekly Scheduling</h2> <p>Implement a scheduling mechanism within the flow to post every two weeks. This ensures a consistent presence on your LinkedIn profile. <img src="/assets/img/Automating LinkedIn Posts/Automating LinkedIn Posts (4).png" alt="Automating LinkedIn Posts"/></p> <p>Step 4 Stay in Control The entire solution is designed to keep you in control. By dynamically creating messages, scheduling bi-weekly posts, and leveraging Power Automate’s capabilities, you can confidently manage your LinkedIn content for the next two years.</p> <h1 id="access-the-power-automate-flow">Access the Power Automate Flow</h1> <p>To implement this solution, you can download the source files directly from <a href="https://github.com/dva81/PowerAutomate">GitHub</a>. Don’t forget to turn the flow on after setting it up.</p> <p>Empower your LinkedIn strategy by taking charge of your posting schedule with this simple yet effective Power Automate Flow. Stay visible, stay engaged, and let automation work for you!</p>]]></content><author><name></name></author><category term="PowerPlatform"/><summary type="html"><![CDATA[Many professionals dedicate time to post regularly or leverage scheduling functions to maintain a consistent online presence. However, when it comes to bulk scheduling posts, LinkedIn lacks a native feature, leaving you in need of a customized solution.]]></summary></entry><entry><title type="html">Mastering AI Model Validation - A Comprehensive Guide for Success</title><link href="https://dva81.github.io/blog/2024/Model-Validation/" rel="alternate" type="text/html" title="Mastering AI Model Validation - A Comprehensive Guide for Success"/><published>2024-01-02T00:00:00+00:00</published><updated>2024-01-02T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/Model-Validation</id><content type="html" xml:base="https://dva81.github.io/blog/2024/Model-Validation/"><![CDATA[<p>AI is everywhere these days! Yet is has existed since the beginning of the computer age. What is so different from 25 years ago? What makes it more powerful, more present? Why all the fuss? Over the last 10 years machine learning has become extremely powerful.</p> <p>Rather than programmers giving machine learning AIs a definitive list of instructions on how to complete a task, the AIs have to learn how to do the task themselves. There are many ways to attempt this, but the most popular approach involves software that is trained by example.</p> <h1 id="the-importance-of-validating-ai-models">The importance of validating AI models</h1> <p>AI is already involved in making important processes (like translations and summarizations) and yet AI is often biased, meaning its recommendations can be too. Time after time, researchers have found that <a href="https://www.newscientist.com/article/2166207-discriminating-algorithms-5-times-ai-showed-prejudice/">neural networks pick up biases from the data sets they are trained on</a>. For example, face recognition algorithms have much lower accuracy for anyone who is not a white man.</p> <p>AI decisions are also opaque or referred to as a “Black box”. How neural networks come to conclusions is very hard to analyze, meaning if they make a crucial mistake, such as missing cancer in an image, it’s very difficult to find out why they made the mistake or to hold them accountable.</p> <blockquote> <p>Validating AI models ensures accuracy, robustness, and reliability.</p> </blockquote> <p>Often creating and validating these sets in a time and resource consuming activity which does not always help the business case due to its labor-intensive nature. But validating AI models is necessary and ensures accuracy, robustness, and reliability.</p> <h1 id="data-splitting-for-model-validation">Data Splitting for Model Validation</h1> <p>The primary reason for data splitting is to avoid <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> , which occurs when the model is too complex and fits the training data too well, resulting in poor generalization performance on new data. <a href="https://machinelearningmastery.com/training-validation-test-split-and-cross-validation-done-right/">By splitting the data, we can ensure that the model is not overfitting to the training data and is generalizing well to new data</a>.</p> <p>Data splitting also provides an unbiased estimate of the model’s performance. If we use the same data for training and testing, the model will perform well on the training data but may not generalize well to new data. By using a separate testing set, we can obtain an unbiased estimate of the model’s performance on new data.</p> <h1 id="determining-validation-set-size">Determining Validation Set Size</h1> <p>To calculate the size of the validation set, there is no one-size-fits-all answer. However, a common approach is to use a 70/30 or 80/20 split for the training and validation sets, respectively. The size of the validation set should be large enough to provide a representative sample of the data but small enough to avoid overfitting. A good rule of thumb is to use at least 5% of the total data for the validation set. That means that for a population of 40000 you need a sample set of 2000.</p> <p>This online calculator helps to calculate sample sizes - <a href="https://www.calculator.net/sample-size-calculator.html">Sample Size Calculator</a> - for test and validation purposes. It provides insights into how the right validation set size contributes to an effective model assessment.</p> <p><img src="/assets/img/Model-validation/model-validation (4).png" alt="online calculator"/></p> <p>Methods of Model Validation: Cross Validation Cross-validation is a technique used in machine learning to evaluate the performance of a model. It involves dividing the dataset into smaller groups and then averaging the performance in each group to reduce the impact of partition randomness on the results. Nevertheless, each method has its own bias toward more positive or negative results. Here are some of the most commonly used cross-validation techniques:</p> <ol> <li> <p>Train-Test Split Method: This method randomly partitions the dataset into two subsets (called training and test sets) so that the predefined percentage of the entire dataset is in the training set. Then, we train our machine learning model on the training set and evaluate its performance on the test set. In this way, we are always sure that the samples used for training are not used for evaluation and vice versa.</p> </li> <li> <p>K-Fold Cross-Validation: In k-fold cross-validation, we first divide our dataset into k equally sized subsets. Then, we repeat the train-test method k times such that each time one of the k subsets is used as a test set and the rest k-1 subsets are used together as a training set. Finally, we compute the estimate of the model’s performance estimate by averaging the scores over the k trials.</p> </li> <li> <p>Leave-One-Out Cross-Validation: In leave-one-out cross-validation, we use all but one sample for training and the remaining sample for testing. We repeat this process for all samples in the dataset and average the performance over all trials.</p> </li> </ol> <p>A Power Platform Example Let us say we have created a model in <a href="https://learn.microsoft.com/en-us/ai-builder/">AI builder</a>, and you want to see if it works as expected. Power Platform comes some standard prediction engines and features, still it is always a good idea to validate the outcome of a process.</p> <p>This technique can also be used with models created by other technologies as well.</p> <p>An easy way to do this in Power platform is with a Power Automate flow.</p> <ol> <li>Put your validation files in one folder. (you can even create them using a generation flow)</li> </ol> <p><img src="/assets/img/Model-validation/model-validation (3).png" alt="generation flow"/></p> <ol> <li> <p>Create a database with all the (meta)data you expect the model to find in the files.</p> </li> <li> <p>Create a flow that reads the files, runs the model and verifies the data found against the data source.</p> </li> <li> <p>Put the results in a file or dashboard.</p> </li> </ol> <p>It can look something like this:</p> <p><img src="/assets/img/Model-validation/model-validation (2).png" alt="flow"/></p> <h1 id="key-takeaways">Key Takeaways</h1> <p>Data splitting is essential for avoiding overfitting and obtaining an unbiased estimate of the model’s performance. It is a crucial step in the validation process. The choice of cross-validation technique depends on the size of the dataset, the number of features, and the computational resources available.</p> <p>These techniques are used to evaluate the performance of a machine learning model and to reduce the impact of partition randomness on the results. It helps ensure the overall accuracy, robustness, and reliability of the business process.</p> <p>Check out <a href="https://machinelearningmastery.com/">Machine Learning Mastery</a> where you’ll find “not so dry” material on this topic.</p>]]></content><author><name></name></author><category term="PowerPlatform"/><category term="AI"/><summary type="html"><![CDATA[AI is everywhere these days! Yet is has existed since the beginning of the computer age. What is so different from 25 years ago? What makes it more powerful, more present? Why all the fuss? Over the last 10 years machine learning has become extremely powerful.]]></summary></entry><entry><title type="html">Boosting Efficiency Docs-as-Code Strategies with Power Platform and Azure DevOps</title><link href="https://dva81.github.io/blog/2023/Docs-as-Code/" rel="alternate" type="text/html" title="Boosting Efficiency Docs-as-Code Strategies with Power Platform and Azure DevOps"/><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2023/Docs-as-Code</id><content type="html" xml:base="https://dva81.github.io/blog/2023/Docs-as-Code/"><![CDATA[<p>Some time ago I took over a Power Platform project right after first deploys to production. The application is a collection of over 30 Power Apps and more than 600 other components. With my DevOps background I was really happy to see the Azure DevOps setup the team had put in place to automate the deploy of the hundreds of Power Platform components as much a as possible.</p> <p>If you want to see how that can be done check out this video by Feline Parein. <a href="https://www.youtube.com/watch?v=pv8CyKrL5ds">Feline Parein - How to deploy Power Platform solution company-wide: Azure DevOps can help with CI/CD - YouTube</a></p> <p>As I was new to the project, I had no context to inform my understanding of what was going on or how things were connected. So, I started reading and looking for information. The developers were working on all the new features and releases, so they had little time to address my information needs.</p> <h1 id="why-docs-as-code">Why docs-as-code?</h1> <p>I found a maze of documents, snippets, designs and much more without any solid structure. Everything was there but it was in chaos. After talking to the team, I found out that, like most developers they don’t like to write documentation, switching tools and work environments, there was no review process, and they did not see the purpose of it as “everything we build is Low-Code”.</p> <blockquote> <p>My next mission was clear: improve the process, improve the documentation.</p> </blockquote> <p>With docs-as-code, you treat your documentation in the same way as your code. A quick start:</p> <ul> <li> <p>Format: pick a format that is easily versioned and transformed like .md. (Markdown)</p> </li> <li> <p>Versioning: Use a versioning system like GIT just as you would with code or configuration files. No need to append a date or v2.2x at the end.</p> </li> <li> <p>Automation: Transform the docs to any format needed (html, pdf, …) and distribute them in one go.</p> </li> <li> <p>Validation: Use an approval flow to get the documents approved and signed off.</p> </li> </ul> <h1 id="how-did-we-do-this">How did we do this?</h1> <p>The good thing was the team already tried to put documentation in repositories. We built on those first steps and completed the process.</p> <ul> <li>Create a wiki and publish it as code</li> </ul> <p><img src="/assets/img/Docs-as-code/Docs-as-Code1.png" alt="publish it as code"/></p> <ul> <li>.md as format. Markdown is a lightweight markup language that you can use to add formatting elements to plaintext text documents.</li> </ul> <p><img src="/assets/img/Docs-as-code/Docs-as-Code2.png" alt="publish it as code"/></p> <p><img src="/assets/img/Docs-as-code/Docs-as-Code3.png" alt="publish it as code"/></p> <ul> <li>In the pipeline step the output is generated in pdf format using a standard task in Azure DevOps. and attached to the artifact for deployment.</li> </ul> <p><img src="/assets/img/Docs-as-code/Docs-as-Code4.png" alt="publish it as code"/></p> <ul> <li>Releasing the documentation. In this step we automated the release to the right location for validation. After approval in Azure Devops that version was copied to the end user or production locations in the Power Platform solution.</li> </ul> <p><img src="/assets/img/Docs-as-code/Docs-as-Code6.png" alt="publish it as code"/></p> <p><img src="/assets/img/Docs-as-code/Docs-as-Code5.png" alt="publish it as code"/></p> <h1 id="bringing-it-all-together">Bringing it all together</h1> <blockquote> <p>“Happy cows make better milk.”</p> </blockquote> <p>By improving the overall documentation process, the team is now much more inclined to write documentation and maintain it. Connecting other people to the project is now much easier and we created better continuity for our client as well.</p> <hr/> <p>If you are more interested in the view of a technical writer, you might want to start your journey at http://writethedocs.org.</p> <hr/> <p>My focus is on structuring, automating and managing business processes using Agile and DevOps best practices. This creates team working environments where business continuity, transparency and human capital come first. Reach out to me on LinkedIn or check out my GitHub for more tips and tricks.</p>]]></content><author><name></name></author><category term="DevOps"/><category term="PowerPlatform"/><summary type="html"><![CDATA[Some time ago I took over a Power Platform project right after first deploys to production. The application is a collection of over 30 Power Apps and more than 600 other components. With my DevOps background I was really happy to see the Azure DevOps setup the team had put in place to automate the deploy of the hundreds of Power Platform components as much a as possible.]]></summary></entry><entry><title type="html">Agile product owner - key responsibilities in scrum development teams</title><link href="https://dva81.github.io/blog/2023/Agile-product-owner/" rel="alternate" type="text/html" title="Agile product owner - key responsibilities in scrum development teams"/><published>2023-11-12T00:00:00+00:00</published><updated>2023-11-12T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2023/Agile-product-owner</id><content type="html" xml:base="https://dva81.github.io/blog/2023/Agile-product-owner/"><![CDATA[<p>Working at ACOLAD as an ECM Team Coach, most of my time is spent leading product teams in implementing common-of-the-shelf software (COTS) applications in enterprise environments. This includes introducing agile software development methodologies and connecting business users and developers in one t-shaped, multifunctional team.</p> <p>When looking at well performing scrum teams, one thing is clear: the product owner (PO) is the key to a successful product while the team is the locked treasure chest.</p> <h1 id="key-responsibilities-of-the-agile-product-owner">Key responsibilities of the Agile product owner</h1> <p>The PO plays one of the three roles (scrum master, dev team and product owner) of the scrum framework for agile project management. He s the only person responsible for managing the Product Backlog, put simply, the list of all things that needs to be done within the project. These can be either technical or user-centric, e.g. in the form of user stories (US). Clearly expressing product backlog items and translating stakeholder needs into usable instructions for creating and delivering solutions to those challenges. This is key to successfully starting any agile software development project.</p> <p>In many cases, business stakeholders on the client side are not ready to level with the tempo of the agile team and will not deliver information and requirements in a way the development team easily understands on what needs to be done. To connect the two, the PO must ensure that the Product Backlog is visible, transparent, clear to all and shows what the team will work on next to complete the goal or deadline!</p> <p>So how can an agile product owner gain trust from his team and value for the client in any software development project? As per my experience, I have listed the top 4 areas in which a Product Owner must perform well to keep scrum teams effective:</p> <ol> <li>You are a leader!</li> </ol> <p>You lead the product development… getting the most value for the client. Agile dictates a self-organizing team and the product owner is second servant-leader in the team, next to the Agile lead or Scrum master. Always deliver added business value even if the development team wants to clean up technical debt. Get grip on your resources and lead them to a better product, but also be aware that code/infra is a living organism.</p> <ol> <li>Product Purpose - Mission</li> </ol> <p>Know your product/mission in and out. Breathe it, live it. Envision it.</p> <p>Focus more on the team instead of business stakeholders. Stakeholders are important but ensure you don t lose the grip on the dev team!</p> <p>Take ownership of the project and demos, don t rely on the dev team to show what was accomplished as a team. Get involved in all the processes from deployment to production and also in maintenance operations, gathering necessary feedback to continuously improve.</p> <ol> <li>Know your backlog</li> </ol> <p>The team can only start working if they clearly know what to do. Backlog management must be done properly. The PO must challenge the development team to improve the backlog, connect with the target audience (users) and log progress in an issue tracker. </p> <p>What is important? Prioritize, not everything is priority 1.</p> <p>Incident control is not the same as the introduction of a new feature or US and vice versa, so they cannot be simply pulled in a sprint. Get your stories ready by making sure User Acceptance Tests (UAT) and operations acceptance criteria are clear.</p> <ol> <li>Delivering quality</li> </ol> <p>Inspect and adapt! This is something we hear often in Agile. The PO is responsible for the value that will be delivered by the whole team, but value also means quality. There is no value if the users don t want to accept it and the product is not up to standards.</p> <p>The PO should check or inspect all aspects of Definition of Done. Are they realistic? Make sure that the project s priorities are well understood by the software development teams. In many cases the software development team only delivers what they think is important. Sure, communication is there, but people tend to work on what they like best and not on what should be done. The PO must keep a close eye on what will be delivered.</p> <h1 id="conclusion">Conclusion</h1> <p>The Agile product owner is the second servant leader in the scrum team, with focus on the word leader . Whether you re in an agile methodology or not, you know the role of the Product Owner is instrumental for the success of any software development project. In more unexperienced teams, where project management is evolving towards more agile practices and an agile mindset, the PO can make an even bigger difference in carrying the mission to deliver a high-quality solution.</p> <p>Updated 12/01/2022</p>]]></content><author><name></name></author><category term="Agile"/><category term="Leadership"/><summary type="html"><![CDATA[Working at ACOLAD as an ECM Team Coach, most of my time is spent leading product teams in implementing common-of-the-shelf software (COTS) applications in enterprise environments. This includes introducing agile software development methodologies and connecting business users and developers in one t-shaped, multifunctional team.]]></summary></entry><entry><title type="html">Content Capture the human factor - exploring user adoption</title><link href="https://dva81.github.io/blog/2023/Content-Capture/" rel="alternate" type="text/html" title="Content Capture the human factor - exploring user adoption"/><published>2023-10-11T00:00:00+00:00</published><updated>2023-10-11T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2023/Content-Capture</id><content type="html" xml:base="https://dva81.github.io/blog/2023/Content-Capture/"><![CDATA[<p>Advances in data capture and document recognition technologies are leading more and more organizations to integrate them into their content management solutions. Embedded with advanced artificial intelligence (AI) capabilities, these systems scan all incoming communications, import documents, recognize and classify data, images or videos. But it doesn’t end there. After the content type is identified, classified and data captured in a digital format, this information is routed within the business environment.</p> <p>While all this is happening, the AI engine enriches the meta-data by reading the content and making decisions as if they were done by a human.</p> <p>Enterprise capture management empowers all employees to access the right content at the right time, by streamlining the lifecycle of their information. This means that from data capture to distribution and archiving, your organization is able to virtually eliminate paper-based information and enhance content visibility. Not forgetting to combine the necessary content compliance and information security functionalities.</p> <p>Surprisingly, the lack of effective adoption is the single largest factor impacting the ROI of implementing enterprise capture technologies. From a technical perspective, most requirements are feasible. Robotics, automation tooling and content intelligence are becoming more and more accessible, requiring less and less in-depth expertise on each technology. However, reluctant users who avoid bringing the new technology into their day-to-day tasks means you re not making the most of your technology investment.</p> <h1 id="the-resistance">The resistance</h1> <p>User adoption is oddly related to resistance to change.</p> <p>Although the premise of user adoption is changing from an outdated or redundant system to a newer, more efficient one, getting everyone on board isn’t as easy as you would expect.</p> <p>Here are a few tips I’ve picked up over the years that can help you get through these challenges.</p> <ol> <li>Document your content management processes</li> </ol> <p>Take implementing content management process automation powered by artificial intelligence as an example. One of the biggest challenges is getting users who know every step of the (manual) process to put it in words.  Generally this isn’t documented and it s the key to identify what steps can be automated and where AI can help make their job easier.</p> <p>To prevent this, make sure your organization is documenting processes by using models and diagrams but keep them simple and understandable so that those with no knowledge of business process management (BPM) can understand the workflows and suggest improvements.</p> <ol> <li>Follow the KISS principle</li> </ol> <p>Implementing content capture technologies can seem daunting for organizations with large amounts of critical information based on paper and no in-house technical background or know-how.</p> <p>It also takes a lot of effort, starting from the teams involved in gathering requirements, to the consultants and AI engineers implementing the solution. All AI is data-driven, so configuring these types of knowledge management solutions is labor intensive due to the sheer amount of data samples it demands.</p> <p>By promoting an agile iterative <a href="https://blog.amplexor.com/taking-fast-track-content-management-projects">implementation approach</a> we start with a very small reference set, work on that, and then move to production as soon as possible. Receiving input from all the teams involved in the project and keeping them up-to-date on progress and milestones is key. Users need to feel encouraged from the first iteration and throughout the development and implementation stages keeping it simple and straightforward helps.</p> <ol> <li>Make sure your teams are involvedHaving worked as an ECM Consultant for organizations of all sizes for nearly ten years, I can say that most content capture solution implementations are technology driven. There s usually no clear change management strategy to ensure smooth adoption on the end-user level.</li> </ol> <p>Sure, the company probably has a business case and a business sponsor, but when it comes to communicating to users what s changing and how and getting them excited about the project, it s normally a reactive event. Very often we see user adoption initiatives being limited to assigning a team member to handle User Acceptance Testing (UAT), after which the system is implemented, and an educational video is expected to do the job.</p> <p>End-users of any technology you re looking to implement should be involved from the early stages of the project.</p> <ol> <li>Use a timely, frequent and transparent communication</li> </ol> <p>What is the purpose of process automation and expected improvements in day-to-day work? Have specific timelines been set or will the implementation be completed in phases?</p> <p>Sometimes the business case is just not clear. And while you don t need to share every detail about the project, communicating key information with your teams early and often will keep them in the loop and allow them to know what to expect.</p> <p>This happened on a project I worked on. After requirements intake and building the content capture solution, we moved to UAT. An end-user started working with the application and all its functionalities: automated data extraction and classification. Basically text boxes and fields are automatically populated so less manual input is needed from the user. Suddenly, she said: If this is automated, what am I going to do all day?  She had no clue what was going on because the organization didn t focus on change management.</p> <p>&lt; “Success consists of going from failure to failure without loss of enthusiasm.”</p> <ul> <li>Winston Churchill</li> </ul> <p>Focus on your strategy’s goal, make it clear and refer to it often. Content capture, artificial intelligence, natural language processing and machine learning are all intimidating subjects in the workplace with a lot of associated misconceptions. Remember that the human factor drives the success of any technology deployment - after all, success is measured by the extent to which people embrace and incorporate new practices and behaviors into their daily work routines, how they adopt and adapt the technology for their own purpose.</p> <p>First published 14/01/2022 - https://blog.acolad.com/</p>]]></content><author><name></name></author><category term="Capture"/><category term="Capture"/><summary type="html"><![CDATA[Advances in data capture and document recognition technologies are leading more and more organizations to integrate them into their content management solutions. Embedded with advanced artificial intelligence (AI) capabilities, these systems scan all incoming communications, import documents, recognize and classify data, images or videos. But it doesn't end there. After the content type is identified, classified and data captured in a digital format, this information is routed within the business environment.]]></summary></entry><entry><title type="html">Continuous delivery with EMC Captiva document capture software</title><link href="https://dva81.github.io/blog/2023/Continous-delivery/" rel="alternate" type="text/html" title="Continuous delivery with EMC Captiva document capture software"/><published>2023-09-11T00:00:00+00:00</published><updated>2023-09-11T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2023/Continous-delivery</id><content type="html" xml:base="https://dva81.github.io/blog/2023/Continous-delivery/"><![CDATA[<p>Setting up a continuous delivery environment can be a daunting task, especially when working with third party software. In this blog post, Dennis Van Aelst and Stef Kusters provide some insight in using EMC Captiva in a continuous delivery environment.</p> <h1 id="what-is-emc-captiva">What is EMC Captiva?</h1> <p>The EMC Captiva products are used to automate document capture and enterprise capture in other words to automatically digitize paper documents such as files or letters. With EMC Captiva, information that was stored on paper becomes easily accessible in content management systems.</p> <h1 id="what-is-continuous-delivery-and-what-are-the-benefits">What is continuous delivery and what are the benefits?</h1> <p>A continuous delivery environment is set up to optimize development processes: it focuses on rapid development and deployment of bite-sized pieces of software.</p> <p>In practice, that means that developers work on the software on their own workstations, while the continuous delivery system takes care of versioning, testing and deploying their work. It reduces the amount of manual tasks, which improves the quality of development and deployment, while reducing unnecessary stress during deployment.</p> <p>Setting up this environment requires a fundamental change in the process, technologies used and mindset of all parties involved. In this post, we ll focus on the process and technologies, as hanging the mindset of people is a different (and much more subjective) challenge, which probably deserves its own blog post.</p> <h1 id="how-we-set-up-a-continuous-delivery-process">How we set up a continuous delivery process</h1> <p>In this post, we ll use the process we created for one of our customers as an example. The image below shows how the set-up of the continuous delivery environment.</p> <p>These are the main steps in this set-up:</p> <p>1.      Developers work at their workstations with Captiva Designer and commit code to the GIT repository</p> <p>2.      Creation of an artifact with Jenkins in Artifactory</p> <p>3.      Testing, acceptance and push to production (the live environment) with Nolio</p> <p>4.      Automated testing using UFT</p> <h1 id="step-1-development-in-captiva-designer-and-commitment-to-git">Step 1: development in Captiva Designer and commitment to GIT</h1> <p>The first step in the process, as shown in the image above, is the developers working on their workstations. They use the Captiva Designer to make changes to the development environment as they please. When they are done, the developers commit their code to the GIT repository. GIT is primarily used for version control in this set-up.</p> <p>If you d like a more detailed description of this step, please read our <a href="http://community.emc.com/people/dennisvanaelst/blog/2016/06/21/how-to-use-captiva-in-a-continuous-delivery-environment">blog post on the EMC Community blog</a>.</p> <h1 id="step-2-creation-of-an-artifact-with-jenkins-in-artifactory">Step 2: Creation of an artifact with Jenkins in Artifactory</h1> <p>In this second step, we focus on version control and getting our deployment configuration in Artifactory using Jenkins. We use GITBLIT as a repository and Tortoise as an interface for the developer.</p> <p>After this step Jenkins kicks off the Nolio process and deployment to the test environment is on its way.</p> <p>A more in-depth description of this step can be found in this <a href="http://community.emc.com/people/dennisvanaelst/blog/2016/06/27/how-to-use-captiva-in-a-continuous-delivery-environment-using-git-and-jenkins">blog post</a>.</p> <h1 id="step-3-testing-acceptance-and-push-to-production-with-nolio">Step 3: Testing, acceptance and push to production with Nolio</h1> <p>Nolio is a configurable release management tool that can be used to deploy to multiple environments and create the automatic deployment scripts, replacing the manual cookbook.</p> <p>The setup we made only uses the deployment part for automating various steps in the deploy process. With Powershell or something similar you can achieve the same thing. We have a template in Nolio for deploying these configurations behind this template are multiple Nolio processes with all the steps needed to deploy processes or profiles or configuration items.</p> <p># Step 4: Using UFT for automated testing</p> <p>At the end of the deployment process, we kick off an automated test using Unified Functional Testing (HP). This test creates a batch by importing images, then indexes them and verified the data in Captiva. We added a web service which automatically looks up values in the batch and compares them to the expected values. That way we can easily create 100 or more test cases.</p> <p>The content of this blog post appeared across various blog posts by Dennis Van Aelst and Stef Kusters on the <a href="http://community.emc.com/people/dennisvanaelst/content?filterID=contentstatus%5Bpublished%5D~objecttype~objecttype%5Bblogpost%5D">EMC Community blog</a> first in October, 2016</p>]]></content><author><name></name></author><category term="Capture"/><category term="DevOps"/><summary type="html"><![CDATA[Setting up a continuous delivery environment can be a daunting task, especially when working with third party software. In this blog post, Dennis Van Aelst and Stef Kusters provide some insight in using EMC Captiva in a continuous delivery environment.]]></summary></entry><entry><title type="html">DevOps and why it should matter when selecting your technology partner</title><link href="https://dva81.github.io/blog/2023/DevOps-and-why-it-matters/" rel="alternate" type="text/html" title="DevOps and why it should matter when selecting your technology partner"/><published>2023-08-11T00:00:00+00:00</published><updated>2023-08-11T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2023/DevOps-and-why-it-matters</id><content type="html" xml:base="https://dva81.github.io/blog/2023/DevOps-and-why-it-matters/"><![CDATA[<p>You might have heard the term DevOps and wondered if it is a new process, a new technology, a job title or something entirely different. Or maybe you re more tech-savvy and automatically thought about continuous delivery, agile and infrastructure as code. But it is much more than that.</p> <h1 id="what-is-devops">WHAT IS DEVOPS?</h1> <p>Definitions vary, but as the name suggests, DevOps integrates the two traditionally separate development and IT operations teams in order to emphasize efficient communication to improve collaboration and productivity.</p> <blockquote> <p>“DevOps is the practice of operations and development engineers participating together in the entire service lifecycle, from design through the development process to production support.”  <br/> <a href="https://theagileadmin.com/what-is-devops/">The Agile Admin</a></p> </blockquote> <p>The books  The DevOps Handbook and The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win complete the definition in three ways :</p> <p><img src="\assets\img\DevOps and why\DevOps and why.jpg" alt="DevOps"/></p> <p>1. Emphasizes the performance of the entire system, i.e. to increase frequency of deployments and improves the time to implement changes or new solutions.</p> <p>2. Is about creating the right to left feedback loops for successfully monitoring, measuring and improving operations every day.</p> <p>3. Focuses on creating a culture that fosters innovation through experimentation and continuous learning.</p> <p>The ultimate goal of DevOps is focusing on the customers needs and their business outcomes, and on satisfying that need with each launch or update. This is achieved with a culture built around the quick release of high-quality technology applications. As the <a href="https://puppet.com/resources/whitepaper/state-of-devops-report">2017 State of DevOps Report</a> helps to quantify, DevOps allows business solution providers to deploy code 46 times more frequently with 440 times faster lead times; have 5 times lower change failure rates and recover from downtime 96 times faster.</p> <p>But how does this turn into advantages for customers? Why should companies draw their attention to DevOps oriented technology partners?</p> <h1 id="key-benefits-for-customers">KEY BENEFITS FOR CUSTOMERS</h1> <p>I am a DevOps Evangelist and became team coach about a year ago. The team consists of technical and business consultants with focus on ECM product implementations (OpenText s Captiva, Documentum and InfoArchive, Microsoft and BOX). We were delivering projects on time and within budget but morale was not on the expected level, so it became clear pretty fast that our project methodology could use a refresh. We set out to introduce DevOps and change the culture and way of working in our company.</p> <p>DevOps is all about improving the flow from development to customer and back, to help us deliver the best possible results and overall experience to the end-users. The key benefits of DevOps which we have experienced are:</p> <h1 id="fasterimplementation-of-new-products-and-features">FASTER IMPLEMENTATION OF NEW PRODUCTS AND FEATURES</h1> <p>By writing software in small chunks that are integrated, tested, monitored and deployed usually in hours versus weeks or months in traditional methodologies  we are able to implement changes or offer new solutions faster and more often. In short, we improved our ability to solve customers Enterprise Content Management challenges and reciprocated to their needs efficiently.</p> <h1 id="high-quality-and-more-stability">HIGH QUALITY AND MORE STABILITY </h1> <p>More speed didn t compromise on quality. While continuous delivery by itself is a high quality service indicator, we were also able to reduce unplanned work and rework, as well as the time spent dealing with security issues. The cross-disciplinary approach of DevOps maximizes reliability in all areas. Our higher response rate to any requests also adds up to increased stability, which is key for any business platform.</p> <h1 id="quicker-detection-and-resolution-of-problems">QUICKER DETECTION AND RESOLUTION OF PROBLEMS</h1> <p>Thanks to better communication and collaboration between operations, software development and customer teams, we improved the feedback loops to quickly mitigate software defects at any stage of the development cycle. In the event of a new release crashing or otherwise disabling the current systems, we were also able to accelerate recovery from downtime. This allows us to proactively identify and resolve vulnerabilities before they impact our customers.</p> <h1 id="reduced-human-errors">REDUCED HUMAN ERRORS</h1> <p><a href="https://puppet.com/resources/whitepaper/state-of-devops-report">Research</a> shows approximately 30% of the development process can be automated, including configuration management, testing, deployments and change approval processes. Replacing cookbooks for all these tasks with automated procedures helps in getting a steady output to our customers, by having the code base in a ready-to-deploy state, and reduces manual mistakes which could otherwise happen.</p> <h1 id="increased-added-value-and-innovation">INCREASED ADDED VALUE AND INNOVATION</h1> <p>By reducing change failure rates and downtime recovery times, our cross-functional teams had more time to collect and implement customer feedback, even at early stages. From these inputs, they could change requirements or specifications, giving them more room to innovate and provide more value to the customer. Improved communication, knowledge sharing and experimentation implicit in DevOps also spreads innovative ideas more easily through all the customer projects.</p> <h1 id="improved-employee-attitude">IMPROVED EMPLOYEE ATTITUDE</h1> <p>We also used a Guild approach, which provides a horizontal communication layer across our Product Engineering teams. Our engineers, testers, and other staff use them to set their own missions, to establish technical roadmaps, to take on joint tasks and to promote knowledge sharing. This initiative, combined with the increased involvement of all in the development lifecycle, leads to happier, more collaborative and productive teams, and this culture shift is likely to spread outside the company.</p> <h1 id="conclusion">CONCLUSION</h1> <p>As businesses pursue digital transformation strategies, there s an increasing need for rapid and continuous deployment of business platforms such as Content Management and collaboration technologies. These platforms can be synonymous with innovation, and the sooner this innovation gets implemented, the better for the company.</p> <p>The solutions provider s performance becomes crucial to enable customers to achieve their mission and remain competitive, and this is where the DevOps methodology comes to play. You ll get faster updates, which means happier end-users and the ability to provide better customer service. Ultimately, the high performance of your technology partner can be converted into high performance of your business.</p> <p>First published 16/05/2022 - https://blog.acolad.com/</p>]]></content><author><name></name></author><category term="DevOps"/><summary type="html"><![CDATA[You might have heard the term DevOps and wondered if it is a new process, a new technology, a job title or something entirely different. Or maybe you re more tech-savvy and automatically thought about continuous delivery, agile and infrastructure as code. But it is much more than that.]]></summary></entry><entry><title type="html">Unlocking the Potential of the Microsoft Power Platform for Life Sciences</title><link href="https://dva81.github.io/blog/2023/Unlocking-the-Potential/" rel="alternate" type="text/html" title="Unlocking the Potential of the Microsoft Power Platform for Life Sciences"/><published>2023-08-11T00:00:00+00:00</published><updated>2023-08-11T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2023/Unlocking-the-Potential</id><content type="html" xml:base="https://dva81.github.io/blog/2023/Unlocking-the-Potential/"><![CDATA[<p>For over a decade, we have been supporting a Life Science customer with a legacy .NET application. Keeping aging infrastructure running is costly and increasingly unsustainable, with 60 to 80 per cent of corporate IT budgets going towards maintaining legacy systems. Our customer approached us to link the application to their new Quality Management System.</p> <p>Our challenge was to reconstruct outdated custom .NET applications that manipulated documents and integrated with other systems, as well as redirect printing services to Ricoh’s follow-me printing. Meeting strict regulations working for a Life Science company, we needed precise data and documents to support the production process, which included both digital and paper-based solutions. Document templates were stored in the Quality Management System.</p> <p>The solution that we proposed was the development of App based on Microsoft Power Platform, SharePoint, Azure, and Power Apps. The aim was to provide users with the same user experience as the old application, so that a single application would deliver all the relevant information for employees to perform their tasks in an efficient and safe way.</p> <p>The Microsoft Power Platform was selected to build the solution as it allows for quick development of enterprise applications based on low code. We coupled the Power Platform to Customers Quality Management System (QMS) as a database for documents. Power Apps, the low-code development component of the Power Platform, was used to develop a user-friendly application covering all needs.</p> <p>The platform was customizable to end-users’ requirements, and the development cycles were short and fast, allowing feedback from end-users to be implemented quickly. Authorization management was centralized via Azure Active Directory, ensuring that all information displayed in the application could be consulted by authorized employees only, and creating and editing the information was limited to a group of “contributor” employees. An audit trail tracked all interactions in the application.</p> <p>The App on the Power Platform proved to be an amazing tool for companies to digitize and really impact the daily work of employees. The functional power of the solution lay in the fact that everything was nicely integrated with the QMS, the Azure function app, and the Ricoh printer service. The application was easily adopted by employees as it was completely in line with their needs, and the integrated analytics showed that the application was actively being used.</p> <p>From an IT portfolio perspective, since SharePoint was used in combination with the Power App, this also helped towards user adoption of SharePoint and the surrounding Microsoft tools within the company. The relatively short implementation cycle and related limited budget needs were obviously an additional benefit: analysis, design, implementation, and roll-out were all covered in less than 50 man-days of work.</p> <p>Technically, the fact that the Power Platform offered the possibility to build a custom application that is easy to maintain and expand is a great added value. So-called “spaghetti” code (extremely unstructured and non-maintainable code) was thus avoided, making it a future-proof technology.</p>]]></content><author><name></name></author><category term="Power"/><category term="Platform"/><summary type="html"><![CDATA[For over a decade, we have been supporting a Life Science customer with a legacy .NET application. Keeping aging infrastructure running is costly and increasingly unsustainable, with 60 to 80 per cent of corporate IT budgets going towards maintaining legacy systems. Our customer approached us to link the application to their new Quality Management System.]]></summary></entry><entry><title type="html">The Art Of Automation</title><link href="https://dva81.github.io/blog/2023/The-Art-of-Automation/" rel="alternate" type="text/html" title="The Art Of Automation"/><published>2023-07-11T00:00:00+00:00</published><updated>2023-07-11T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2023/The-Art-of-Automation</id><content type="html" xml:base="https://dva81.github.io/blog/2023/The-Art-of-Automation/"><![CDATA[<p>When a friend recently approached me with a question about personal productivity, I found myself thinking, ‘I can certainly offer some guidance on that.’</p> <p>This was her initial message to me:</p> <blockquote> <p>From November I will not work on Wednesdays (my daughter is going to school, and I am taking parental leave). Is there a way my account will send automatic replies, saying our office is closed, when I receive emails on Wednesday? Or do I have to reset this every Wednesday?</p> </blockquote> <p>The way this is described is not a standard outlook feature to my knowledge and research. I wanted to use this example <a href="https://www.linkedin.com/pulse/smart-out-office-microsoft-flow-marc-de-kleijn">Smart Out of Office with Power Automate (updated May 2021) (linkedin.com)</a> but then I saw this new feature in Power Automate and I wanted to give it a go. Microsoft Power Automate is an online workflow service that automates actions across the most common apps and services.</p> <p>Describe it to design it… How does it work?</p> <p>This AI-powered feature uses a language model to create a cloud flow from a description. The model understands natural language input and writes the code for the flow. This experience uses GPT-3 as a foundation to understand a natural language prompt and generate a working cloud flow.</p> <p>Step 1: Copy - Paste - Send</p> <p>I copy/pasted the message I got via chat and clicked on send.</p> <p><img src="/assets/img/The Art of Automation/The Art of Automation (1).png" alt="The Art of Automation"/></p> <p>The first suggestion it gave was not really what I was looking for. It could work but, in my opinion, it did not cover the complete scenario.</p> <p><img src="/assets/img/The Art of Automation/The Art of Automation (2).png" alt="The Art of Automation"/></p> <p>![The Art of Automation]/assets/img/The Art of Automation/The Art of Automation (3).png) Step 2: Do it again</p> <p>So, I tried again. Iteration two.</p> <p>This time I prompted: monitor my mailbox. if I get an email and it is Wednesday reply with an email that says I am not available on Wednesdays. A bit simpler, more straightforward. Notice I put in an if statement to guide the engine.”</p> <p><img src="/assets/img/The Art of Automation/The Art of Automation (4).png" alt="The Art of Automation"/></p> <p><img src="/assets/img/The Art of Automation/The Art of Automation (5).png" alt="The Art of Automation"/></p> <p>This time it made a suggestion that could be a good start. I put a proper reply message and started the flow test. This whole process took about 10 minutes.</p> <p>It may look like a super easy way to create flows, which it is, but I know the Power Platform, can write code and know how to write a decent ChatGPT prompt to get result I can work with. It is my belief that you need to know basic programming structures and have some coding skill to get a decent output out of any AI-powered tool or low-code platform.</p> <p>This is a great assistant for workflow designers, but it does not replace them.</p> <p>That said, I installed the process, my friend made some notes and changed the email message and does not have to worry about her autoreply again. Mission accomplished!</p> <p>Have look at this free online workshop if you are interested in Personal Productivity and automation.</p> <table> <tbody> <tr> <td>[Power Automate: Automation - Online Workshop - Training</td> <td>Microsoft Learn.](https://learn.microsoft.com/en-us/training/paths/robotic-process-automation-online-workshop/)</td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="Power"/><category term="Platform"/><summary type="html"><![CDATA[When a friend recently approached me with a question about personal productivity, I found myself thinking, 'I can certainly offer some guidance on that.']]></summary></entry><entry><title type="html">Unearth Hidden Windows Gems - Mastering Microsoft Steps Recorder for Troubleshooting and Analysis</title><link href="https://dva81.github.io/blog/2023/Unearth-Hidden-Windows-Gem/" rel="alternate" type="text/html" title="Unearth Hidden Windows Gems - Mastering Microsoft Steps Recorder for Troubleshooting and Analysis"/><published>2023-06-10T00:00:00+00:00</published><updated>2023-06-10T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2023/Unearth-Hidden-Windows-Gem</id><content type="html" xml:base="https://dva81.github.io/blog/2023/Unearth-Hidden-Windows-Gem/"><![CDATA[<p>This tool has been in Windows for some time, but a lot of people still don’t know about it. That has to change.</p> <p>Steps Recorder (called Problems Steps Recorder in Windows 7) is a program that helps you troubleshoot a problem on your device by recording the exact steps you took when the problem occurred. You can then send this record to a support professional to help them diagnose the problem. <a href="https://support.microsoft.com/en-us/windows/record-steps-to-reproduce-a-problem-46582a9b-620f-2e36-00c9-04e25d784e47">Microsoft Steps Recorder</a></p> <ul> <li>Steps Recorder doesn t record text that you type (such as a password), except for function and shortcut keys.</li> <li>Some programs, like a full-screen game, might not be captured accurately.</li> </ul> <p>But I think it can be used for more, such as:</p> <ul> <li>Steps recorder can help to map the functional requirements based on steps.</li> <li>It can help you with process mining and process analysis.</li> </ul> <p>I use it when I go to a customer and ask them to demo the process in their software. Instead of recording, I use this because it makes screenshots and records actions.</p> <p>It records steps, actions, click and screenshot and creates a document that you can edit in Word.</p> <p>After adding some text my analysis document is ready.</p> <h1 id="getting-started">Getting started</h1> <p>Open the application</p> <p>you can find the application in the start menu of your Windows. Just type in Step Recorder</p> <p><img src="/assets/img/Unearth Hidden Windows Gems/Unearth Hidden Windows Gems (1).png" alt="Unearth Hidden Windows Gems"/></p> <p>Adjust the number of captures.</p> <p>Set this to 100. Every scroll of the mouse is recorded, so if you are nervous, you’ll have a lot of waste<strong>.</strong></p> <p><img src="/assets/img/Unearth Hidden Windows Gems/Unearth Hidden Windows Gems (2).png" alt="Unearth Hidden Windows Gems"/></p> <p>Start recording.</p> <p>Just click on Start</p> <p><img src="/assets/img/Unearth Hidden Windows Gems/Unearth Hidden Windows Gems (3).png" alt="Unearth Hidden Windows Gems"/></p> <p>After recording</p> <p><img src="/assets/img/Unearth Hidden Windows Gems/Unearth Hidden Windows Gems (4).png" alt="Unearth Hidden Windows Gems"/></p> <p><img src="/assets/img/Unearth Hidden Windows Gems/Unearth Hidden Windows Gems (5).png" alt="Unearth Hidden Windows Gems"/></p> <p>You’ll have the opportunity to save the recorded steps. It does this a .mht file in .zip container. After which you can unpack it and open the .mht file in Word.</p> <h1 id="final-thoughts">Final thoughts</h1> <p>In today’s digital age, having the right tools at your disposal can make all the difference when it comes to solving technology-related issues. Microsoft Steps Recorder is one of those invaluable tools that, despite being around for a while, remains hidden in plain sight for many. As we’ve explored in this post, it can be a game-changer when it comes to troubleshooting and seeking support. So, the next time you encounter a perplexing problem on your Windows device, don’t forget to give Steps Recorder a try. It may just be the key to a quicker and more efficient resolution, bringing you one step closer to smooth, stress-free computing.</p>]]></content><author><name></name></author><category term="Tools"/><summary type="html"><![CDATA[This tool has been in Windows for some time, but a lot of people still don't know about it. That has to change. Steps Recorder (called Problems Steps Recorder in Windows 7) is a program that helps you troubleshoot a problem on your device by recording the exact steps you took when the problem occurred.]]></summary></entry></feed>