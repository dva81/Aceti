<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://dva81.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://dva81.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-19T13:15:10+00:00</updated><id>https://dva81.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">A more advanced pipeline in Azure DevOps for Power Platform Solutions</title><link href="https://dva81.github.io/blog/2024/advanced-pipeline-AzureDevOps/" rel="alternate" type="text/html" title="A more advanced pipeline in Azure DevOps for Power Platform Solutions"/><published>2024-07-19T00:00:00+00:00</published><updated>2024-07-19T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/advanced-pipeline-AzureDevOps</id><content type="html" xml:base="https://dva81.github.io/blog/2024/advanced-pipeline-AzureDevOps/"><![CDATA[<h1 id="a-more-advanced-pipeline-in-azure-devops-for-power-platform-solutions">A more advanced pipeline in Azure DevOps for Power Platform solutions.</h1> <table> <tbody> <tr> <td>When working with many developers or designers in highly secure environment with a lot of compliance rules and regulations, Power Platform can be challenging to maintain. In this [Azure DevOps: Easily deploy a Power platform solution</td> <td>LinkedIn](https://www.linkedin.com/pulse/azure-devops-easily-deploy-power-platform-solution-dennis-van-aelst-mzfpe/?trackingId=Zt9plSeCTg2OyehjOFpuog%3D%3D) article, I gave an example on how to create a simple DevOps pipeline.</td> </tr> </tbody> </table> <p>This time I will show you two advanced pipeline to export and import a Power Platform solution with a GITHUB connection. The thought behind this is that the highly regulated enterprises have more complex working environments and deployment needs.</p> <ul> <li>Many developers are working on the same solution in different development environments.</li> <li>Code or configuration check must be done before deploying preferably automated</li> <li>Branching and merging policies.</li> <li>Standard configurations must be added automatically after development</li> </ul> <h2 id="setting-the-stage">Setting the stage</h2> <p>We will be using the following components.</p> <ul> <li>Azure DevOps environment for the build and release pipelines</li> <li>GITHUB repository for storing the configuration and merging the different brances</li> <li>Power Platform environment with a solution to export</li> </ul> <p>The example will be importing from only one environment but it can be extended if needed. If you are reading this article, I can safely assume you know your way around Microsoft Power Platform. However source control is not always related to low code so if you are new to GITHUB check out this Beginner’s guide to GitHub repositories: [How to create your first repo - The GitHub Blog]</p> <h2 id="build">Build</h2> <p>There are two pipelines. Get the Solution and Pack and drop branches <img src="https://github.com/user-attachments/assets/5dd9d6ee-4239-458a-a15f-5a0e658ed683" alt="image"/></p> <h3 id="get-the-solution">Get the Solution</h3> <p>The goal is to export the solution from the Power Platform environment and store the configuration in GITHUB under a new branch. <img src="https://github.com/user-attachments/assets/1ca58311-b0c7-4854-a3b3-99b42f81540f" alt="image"/></p> <p>The first three jobs are simple but before those start the Checkout – job creates / clones the Github repo on the agent. This way we can use that location to clean the repository before unpacking the solution in that location. <img src="https://github.com/user-attachments/assets/48762fd5-985f-4876-a374-871ae7cd5893" alt="image"/></p> <p>In the Clean repo step, The GIT rm command is used to remove the old configuration making it ready to accept the new incoming. <img src="https://github.com/user-attachments/assets/801024a4-f39b-4178-bbdc-bca224ed6750" alt="image"/></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>write-host "Set location"
Set-Location -Path $(Build.Repository.LocalPath)

write-host "Start GIT Stuff"
git config user.email "$(Build.RequestedForEmail)"
git config user.name $(Build.RequestedForId)

write-host "Switch"
git switch -c $(Build.BuildId)
</code></pre></div></div> <p>After unpacking, we can send the files to GITHUB in a new branch. I am using predefined variables <a href="https://learn.microsoft.com/en-us/azure/devops/pipelines/build/variables?view=azure-devops&amp;tabs=yaml#identity_values">Predefined variables - Azure Pipelines | Microsoft Learn</a> because I like standard basic things. <img src="https://github.com/user-attachments/assets/18453d97-84e8-4584-9471-6ad541149368" alt="image"/></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>write-host "Set location"
Set-Location -Path $(Build.Repository.LocalPath)

write-host "Start GIT Stuff"
git config user.email "$(Build.RequestedForEmail)"
git config user.name $(Build.RequestedForId)

write-host "Switch"
git switch -c $(Build.BuildId)
# git checkout -b $(Build.BuildId)

write-host "Adding"
git status
git add *

write-host "Commit"
git commit -m "$(Build.SourceVersionMessage)"
git status

write-host "Push code to new repo"
git -c http.extraheader="AUTHORIZATION: bearer $(System.AccessToken)" push origin $(Build.BuildId)
git status
</code></pre></div></div> <h3 id="things-did-not-go-as-planned">Things did not go as planned</h3> <p>Some things took more time than others. These types configuration of things are an intricate maze of tools, tasks and settings.</p> <p><strong>The Git push did not work</strong> The Git push did not work and I got a message the “authentication was not done properly”. I missed a setting in the Agent Job. <a href="https://stackoverflow.com/questions/64803872/azure-pipeline-cannot-prompt-because-terminal-prompts-have-been-disabled">git - Azure Pipeline, Cannot prompt because terminal prompts have been disabled - Stack Overflow</a> <img src="https://github.com/user-attachments/assets/35fb26fc-d67a-4037-aab1-64c95ce31b3f" alt="image"/></p> <p><strong>Error Not a repo</strong> This was because I cleaned the repo before filling it again. I solved this with the GIT rm command. Which did not destroy the cloned repo. <a href="https://komodor.com/blog/solving-fatal-not-a-git-repository-error/">Solved: fatal: Not a git repository (or any of the parent directories): .git (komodor.com)</a></p> <p><strong>Unpack vs unzip does not make a difference.</strong> What is strange is that the [Content_types].xml is not extracted in both cases and files in the root are placed in the folder ‘other’. I lost some figering this out. However after packing the Solution, it does create the correct structure again… <img src="https://github.com/user-attachments/assets/36170d33-3513-4a48-965a-6a25f9335d15" alt="image"/></p> <p><img src="https://github.com/user-attachments/assets/9ff41786-dcf3-49e2-809f-e3929c48a452" alt="image"/></p> <p><img src="https://github.com/user-attachments/assets/0f476b28-d247-44d9-babf-87d688717027" alt="image"/></p> <h1 id="pack-and-drop">Pack and drop</h1> <p>This pipeline gets the configuration items from the GITHUB repo and packs the Solution again to a managed solution. out of the box you cannot pack from unmanaged to managed and visa versa. The result is a deployable artifact that can be released to any environment. I did not show the deployment settings in the example. Check out my other article for that. You will need to incorporate all connections references and other dependancies like custom connectors.</p> <p><img src="https://github.com/user-attachments/assets/e500ec17-0284-4b47-8b32-3191725d0e80" alt="image"/></p> <h1 id="release-pipelines">Release pipelines</h1> <p>The release pipeline is easy. We are working with managed solutions. Unmanaged is not advised! Microsoft considers these as still under development, and you can import or export as unmanaged. You can modify unmanaged solutions. Managed solutions are complete solutions ready for distribution that you cannot modify after importing them to the selected environment. These solutions are intended for production environments. More info on: <a href="https://learn.microsoft.com/en-us/power-apps/maker/data-platform/solution-layers">Solution layers - Power Apps | Microsoft Learn</a> <img src="https://github.com/user-attachments/assets/2b2f9846-aa55-40f3-9ad2-734128707a09" alt="image"/></p> <p>By using the managed solution all your environments stay nice and clean.</p> <p>Here is an example of a multi stage environment. You can even make this dynamic if you incorporate pipelines and scripts for environment provisioning and use the API for Azure DevOps to automatically create pipelines and release configurations.</p> <p><img src="https://github.com/user-attachments/assets/0f0f5ffb-4e73-4699-88bd-7f27542f00d6" alt="image"/></p> <h1 id="a-parting-note">A parting note</h1> <p>This way method provides a way to introduce all capabilities of modern version control on all aspects of a Power Platform solution. Beware though. You are merging configurations, not real code. The syntax may be ok but that does not mean it will work! The Power Platform is not really intended to use this way. So be carefull!</p> <p>My focus is on structuring, automating and managing business processes using Agile and DevOps best practices. This creates working environments where business continuity, transparency and human capital come first. Reach out to me on <a href="https://www.linkedin.com/in/dennisvanaelst">LinkedIn</a> or check out my <a href="https://github.com/dva81">github</a> or <a href="https://www.dennisvanaelst.net/">blog</a> for more tips and tricks.</p> <hr/> <p>The ideas and underlying essence are original and generated by a human author. The organization, grammar, and presentation may have been enhanced by the use of AI.</p>]]></content><author><name></name></author><category term="PowerPlatform"/><category term="AI"/><category term="DevOps"/><category term="Copilot"/><summary type="html"><![CDATA[This time I will show you two advanced pipeline to export and import a Power Platform solution with a GITHUB connection. The thought behind this is that the highly regulated enterprises have more complex working environments and deployment needs.]]></summary></entry><entry><title type="html">Skills vs Connectors in Microsoft Copilot Studio - Making the Right Choice</title><link href="https://dva81.github.io/blog/2024/Skills-vs-Connectors/" rel="alternate" type="text/html" title="Skills vs Connectors in Microsoft Copilot Studio - Making the Right Choice"/><published>2024-07-07T00:00:00+00:00</published><updated>2024-07-07T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/Skills-vs-Connectors</id><content type="html" xml:base="https://dva81.github.io/blog/2024/Skills-vs-Connectors/"><![CDATA[<h1 id="skills-vs-connectors-in-microsoft-copilot-studio-making-the-right-choice">Skills vs Connectors in Microsoft Copilot Studio: Making the Right Choice</h1> <p>In the evolving landscape of automation and AI integration, Microsoft Copilot Studio offers two primary methods for enhancing functionality: Skills and Connectors. Understanding the differences, strengths, and best use cases for each can help organizations make informed decisions. This post explores the key aspects of Skills and Connectors, and provides guidance on selecting the right approach for your projects.</p> <h2 id="understanding-connectors">Understanding Connectors</h2> <p>Connectors in the Power Platform provide a low-code solution to integrate various services and applications. This approach is particularly useful for users who want to extend API calls without delving into extensive coding. Here are some key points about Connectors:</p> <ul> <li><strong>Preview Availability</strong>: Connectors are currently available in preview within the Power Platform.</li> <li><strong>Low-Code Approach</strong>: Designed for ease of use, enabling users to integrate services with minimal coding.</li> <li><strong>DLP Policy Impact</strong>: Data Loss Prevention policies apply, with standard limitations allowing 500 calls per minute per connector. These limits are adjustable based on the environment.</li> <li><strong>Authentication</strong>: Proper authentication needs to be verified to ensure secure connections.</li> </ul> <h2 id="exploring-skills">Exploring Skills</h2> <p>Skills offer a more flexible, pro-code approach, requiring Azure infrastructure. This method is ideal for users with coding expertise who need to implement complex customizations. Key features of Skills include:</p> <ul> <li><strong>Pro-Code Flexibility</strong>: Greater flexibility due to the ability to write and modify code.</li> <li><strong>Azure Infrastructure</strong>: Requires setup and management of Azure resources.</li> <li><strong>Extended Capabilities</strong>: Skills can extend further than connectors, especially when paired with Copilot extensions.</li> </ul> <h2 id="user-experience-in-copilot">User Experience in Copilot</h2> <p>Regardless of whether you choose Skills or Connectors, the user experience within Copilot remains largely consistent. Both methods aim to streamline the integration process and enhance functionality.</p> <h2 id="making-the-decision">Making the Decision</h2> <p>For Proof of Concept (POC) use cases, the primary strategy is to use custom connectors. This approach allows for quick integration and testing without extensive coding. However, it’s crucial to continuously evaluate the technical and functional requirements of your project. If the need for more advanced customization arises, considering Skills might be necessary.</p> <h2 id="action-plan">Action Plan</h2> <p>To effectively implement Connectors or Skills, consider the following steps:</p> <ol> <li><strong>Performance Overview</strong>: Assess performance needs and peak usage to estimate the number of API calls required.</li> <li><strong>Limit Adjustments</strong>: Adjust limits as necessary and manage these changes from an Application Lifecycle Management (ALM) perspective.</li> <li><strong>Performance Testing</strong>: Conduct thorough performance testing to ensure the chosen method meets your requirements.</li> </ol> <p>By carefully evaluating your project’s needs and understanding the capabilities of both Skills and Connectors, you can make informed decisions that align with your goals and resources.</p> <hr/> <p>For more detailed information on using Microsoft Bot Framework skills and Power Platform connectors, refer to the official <a href="https://learn.microsoft.com/">Microsoft Copilot Studio documentation</a>.</p> <hr/> <p>My focus is on structuring, automating and managing business processes using Agile and DevOps best practices. This creates working environments where business continuity, transparency and human capital come first. Reach out to me on <a href="https://www.linkedin.com/in/dennisvanaelst">LinkedIn</a> or check out my <a href="https://github.com/dva81">github</a> or <a href="https://www.dennisvanaelst.net/">blog</a> for more tips and tricks.</p> <hr/> <p>The ideas and underlying essence are original and generated by a human author. The organization, grammar, and presentation may have been enhanced by the use of AI.</p>]]></content><author><name></name></author><category term="PowerPlatform"/><category term="AI"/><category term="DevOps"/><category term="Copilot"/><summary type="html"><![CDATA[This article provides a foundational understanding to help you navigate the choice between Skills and Connectors in Microsoft Copilot Studio.]]></summary></entry><entry><title type="html">Streamline Your Time Tracking - How to Use Outlook and Power Automate for Efficient Reporting</title><link href="https://dva81.github.io/blog/2024/Streamline-Your-Time-Tracking/" rel="alternate" type="text/html" title="Streamline Your Time Tracking - How to Use Outlook and Power Automate for Efficient Reporting"/><published>2024-06-30T00:00:00+00:00</published><updated>2024-06-30T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/Streamline-Your-Time-Tracking</id><content type="html" xml:base="https://dva81.github.io/blog/2024/Streamline-Your-Time-Tracking/"><![CDATA[<h1 id="streamline-your-time-tracking---how-to-use-outlook-and-power-automate-for-efficient-reporting">Streamline Your Time Tracking - How to Use Outlook and Power Automate for Efficient Reporting</h1> <p>If you are an organized consultant or developer, you might recognize the screenshot from the Outlook calendar below. It’s color-coded, organized, and includes different topics, meetings, customers, and projects. When working with various customers and projects, you often need to fill in timesheets for each one in multiple systems and formats. I constantly lose track and am not good at handling multi-faceted administration across different tools and software. To simplify this, I devised a straightforward method to log all my activities in Outlook and generate simple weekly reports.</p> <p>Check out my post on LinkedIn <a href="https://www.linkedin.com/pulse/streamline-your-time-tracking-how-use-outlook-power-dennis-van-aelst-4u4ne/?trackingId=Gf8WIHj%2FQG27H8lGUZN38g%3D%3D">Streamline Your Time Tracking: How to Use Outlook and Power Automate for Efficient Reporting</a>) .</p> <hr/> <p>My focus is on structuring, automating and managing business processes using Agile and DevOps best practices. This creates working environments where business continuity, transparency and human capital come first. Reach out to me on <a href="https://www.linkedin.com/in/dennisvanaelst">LinkedIn</a> or check out my <a href="https://github.com/dva81">github</a> or <a href="https://www.dennisvanaelst.net/">blog</a> for more tips and tricks.</p> <hr/> <p>The ideas and underlying essence are original and generated by a human author. The organization, grammar, and presentation may have been enhanced by the use of AI.</p>]]></content><author><name></name></author><category term="PowerPlatform"/><category term="AI"/><summary type="html"><![CDATA[When working with various customers and projects, you often need to fill in timesheets for each one in multiple systems and formats. I devised a straightforward method to log all my activities in Outlook and generate simple weekly reports.]]></summary></entry><entry><title type="html">Azure DevOps - Easily deploy a Power platform solution</title><link href="https://dva81.github.io/blog/2024/deploy-Power-platform-solutions/" rel="alternate" type="text/html" title="Azure DevOps - Easily deploy a Power platform solution"/><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/deploy-Power-platform-solutions</id><content type="html" xml:base="https://dva81.github.io/blog/2024/deploy-Power-platform-solutions/"><![CDATA[<h1 id="azure-devops--easily-deploy-a-power-platform-solution">Azure DevOps : Easily deploy a Power platform solution</h1> <p>I have been an Azure DevOps enthusiast for years. The ease of use and flexibility of the platform are super. In this article I am sharing my not-so-complex take on deploying a Power Platform Solution across environments. I will explain how to use a repo, build pipeline and release to different environments.</p> <h2 id="repos">Repos</h2> <p>There is no need to download the solution in a repo if you are not going to manipulate the source / zip file. Using the repo for the deployment settings json file is a good practice as these are configurations that are not managed in the solution. This way you can have version control over the deployment settings. The solution versions are managed in Power Platform or you can roll back from the artifact.</p> <p><img src="https://github.com/dva81/dva81.github.io/assets/65031840/9ed4499a-b617-44de-9df4-0c93aeb1bd00" alt="image"/></p> <h2 id="pipelines----build-artifact">Pipelines - build artifact</h2> <p>I see pipelines as a means to create a deployable artifact of deployable item. So for me it can be limited to the steps to collect and package the components that need to be deployed. You can either schedule this or start this manually if you are ready to deploy.</p> <p><img src="https://github.com/dva81/dva81.github.io/assets/65031840/674d2575-bf4e-4d2b-8230-dc44428dd29c" alt="pipeline actions"/></p> <p><img src="https://github.com/dva81/dva81.github.io/assets/65031840/8502521d-d641-4a56-b9ea-ff25f860f638" alt="artifact content"/></p> <p>This package contains everything you need to deploy to any environment. So even if you want to roll back the development environment you can do it with this package.</p> <h2 id="pipelines----release">Pipelines - Release</h2> <p>The release stages are set up from the build output / artifact. Everything is in the artifact, there is no need for other sources. In this example we have three environments to deploy. This can be across tenants if needed. We use the service connections for the connections.</p> <p><img src="https://github.com/dva81/dva81.github.io/assets/65031840/96a44216-f9fd-4ec3-8ab2-7828c6861566" alt="release stages"/></p> <p>As far as actions go. Always install the tool on the agent and import the solution using the deployment settings json file we configured in the repo.</p> <p><img src="https://github.com/dva81/dva81.github.io/assets/65031840/9ef2c0b1-f4e7-4589-bc81-546229476b69" alt="stage actions"/></p> <p><img src="https://github.com/dva81/dva81.github.io/assets/65031840/0f5fb92b-1154-436f-8821-8336a7ca3347" alt="deployment settings"/></p> <h1 id="thats-it-no-need-to-overcomplicate">That’s it. No need to overcomplicate.</h1> <p>Hope this helps! Feel free to reach out on LinkedIn.</p> <p>Also check out this post if you are interested in Azure DevOps! <a href="https://www.dennisvanaelst.net/blog/2023/Docs-as-Code/">Boosting Efficiency Docs-as-Code Strategies with Power Platform and Azure DevOps</a> or this https://learn.microsoft.com/en-us/shows/devops-lab/how-to-deploy-power-platform-with-azure-devops</p> <p>My focus is on structuring, automating and managing business processes using Agile and DevOps best practices. This creates working environments where business continuity, transparency and human capital come first. Reach out to me on <a href="https://www.linkedin.com/in/dennisvanaelst">LinkedIn</a> or check out my <a href="https://github.com/dva81">github</a> or <a href="https://www.dennisvanaelst.net/">blog</a> for more tips and tricks.</p> <hr/> <p>The ideas and underlying essence are original and generated by a human author. The organization, grammar, and presentation may have been enhanced by the use of AI.</p>]]></content><author><name></name></author><category term="PowerPlatform"/><category term="AI"/><category term="DevOps"/><summary type="html"><![CDATA[In this article I am sharing my take on deploying a Power Platform Solution across environments. How to use a repo, build pipeline(s) and release to different environments.]]></summary></entry><entry><title type="html">Document Processing with AI Builder - A Practical Guide to the document automation toolkit</title><link href="https://dva81.github.io/blog/2024/AI-builder-quick-guide/" rel="alternate" type="text/html" title="Document Processing with AI Builder - A Practical Guide to the document automation toolkit"/><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/AI-builder%20quick%20guide</id><content type="html" xml:base="https://dva81.github.io/blog/2024/AI-builder-quick-guide/"><![CDATA[<h1 id="document-processing-with-ai-builder-a-practical-guide-to-the-document-automation-toolkit">Document Processing with AI Builder: A Practical Guide to the document automation toolkit</h1> <p>In today’s fast-paced digital landscape, document automation is an essential tool for businesses looking to improve efficiency and accuracy. Microsoft’s AI Builder provides a comprehensive solution for automating document processing, integrating seamlessly with other Microsoft services like SharePoint and Teams. This blog post delves into the workings of the Document Automation Toolkit within AI Builder, detailing the steps involved in setting it up and the adjustments necessary to tailor it to specific needs.</p> <h2 id="getting-started-out-of-the-box-functionality">Getting Started: Out-of-the-Box Functionality</h2> <p>You can access the document automation toolkit in Power Automate. Learn about the toolkit on <a href="https://learn.microsoft.com/en-us/ai-builder/doc-automation">Document automation toolkit - AI Builder | Microsoft Learn</a>. The inital installation is standard and next-next-finish. You do need some knowledge of Power Platform if you want to get started with this.<br/> The two installations I did stalled on the last step but everything looked in working order after a refresh of the browser.</p> <h3 id="1-setting-up-the-solution">1. Setting Up the Solution</h3> <p>To begin, create a new solution within the Power Platform environment and import all toolkit components. The Toolkit comes as a managed solution, so this way you can make the necesary adjustments to the flow’s, apps and dataverse tables. This step is straightforward, enabling users to swiftly set up a foundation for their document automation process. <img src="https://github.com/dva81/dva81.github.io/assets/65031840/6f6d19a2-4ba8-48f5-b961-6a53b5bc43bf" alt="image"/></p> <h3 id="2-model-configuration">2. Model Configuration</h3> <p>Next, create a simple model to complete the initial configuration steps within the app. This involves defining the types of documents to be processed and the data fields to be extracted. <img src="https://github.com/dva81/dva81.github.io/assets/65031840/9eb347fd-6b54-44da-833c-067df788d422" alt="image"/></p> <h3 id="3-base-flow-testing">3. Base Flow Testing</h3> <p>Test the base flow by integrating it with an email system. This initial test ensures that the basic document processing pipeline is functioning correctly.</p> <p>Check and test the email importer flow <img src="https://github.com/dva81/dva81.github.io/assets/65031840/70e4aa74-1942-475f-8547-48d6892ab21c" alt="image"/></p> <p>Configure the Power App to use the correct model. <img src="https://github.com/dva81/dva81.github.io/assets/65031840/4d4fd77b-46be-4219-9fa6-5b3ecc7425d4" alt="image"/></p> <p>Ready to go! <img src="https://github.com/dva81/dva81.github.io/assets/65031840/6b63ee00-ae52-4057-967d-47efda211e02" alt="image"/></p> <h3 id="4-integrating-file-imports">4. Integrating File Imports</h3> <p>Adjust the flow to accommodate file imports from SharePoint or Teams. This integration allows for a seamless transition of documents from these storage solutions into the AI Builder processing pipeline. For this I duplicated the email importer flow and adjusted the steps in the flow to retrieve the files from a sharepoint location.</p> <p>It took only a few hours to get the document automation application up and running. After processing a few documents a few opportunities for improvement emerged.</p> <h2 id="customizing-features-for-enhanced-usability">Customizing Features for Enhanced Usability</h2> <h3 id="1-enhancing-the-user-interface">1. Enhancing the User Interface</h3> <p>The out-of-the-box user interface (UI) includes an interactive table, which may not be entirely user-friendly for all scenarios like high volume input or table manipulation. To improve this, custom buttons and actions were created within the app, enhancing the overall user experience (UX) and making the toolkit more intuitive. I added an add, delete and copy row button and a general save button. The copy function is great if lines have similar text in it and were not extracted properly. And the lay-out was tweaked a bit. The pdf viewer was made a bit smaller to allow for more room for the table and fields. <img src="https://github.com/dva81/dva81.github.io/assets/65031840/22280ec1-a86a-48e5-9f5a-f66a4463c785" alt="image"/></p> <h3 id="2-overcoming-table-structure-challenges">2. Overcoming Table Structure Challenges</h3> <p>In cases where a complex table structure needs to be extracted over multiple pages, the default capabilities of AI Builder might fall short. To address this, optical character recognition (OCR) and a series of regular expressions were employed. This combination allowed for the extraction of complex data structures from documents, ensuring no information was lost during processing.</p> <h3 id="3-addressing-field-restrictions">3. Addressing Field Restrictions</h3> <p>The default character length restriction in Dataverse tables is set to for example 100 characters. For our model, longer field lengths were necessary. Adjusting these settings in Dataverse was possible but initially caused errors during testing. By tweaking these settings and running multiple tests, the issues were eventually resolved, allowing for smoother data processing. <img src="https://github.com/dva81/dva81.github.io/assets/65031840/8d4fa078-950d-4176-a25f-3801a5f67ecd" alt="image"/></p> <h3 id="4-managing-bulk-deletion">4. Managing Bulk Deletion</h3> <p>Bulk deletion of documents is not supported out of the box. To manage this, a manual query was created for document deletion within Dataverse tables. This workaround ensured that unnecessary documents could be efficiently removed without manual intervention.</p> <h2 id="conclusion">Conclusion</h2> <p>Microsoft’s AI Builder and Document Automation Toolkit offers robust features for automating document processing tasks. While the out-of-the-box functionalities provide a solid starting point, specific customizations and adjustments can significantly enhance usability and performance. By addressing UI challenges, overcoming complex table extraction issues, modifying field restrictions, and implementing manual deletion queries, businesses can tailor the AI Builder to meet their unique requirements, ensuring a more efficient and streamlined document automation process.</p>]]></content><author><name></name></author><category term="PowerPlatform"/><category term="AI"/><summary type="html"><![CDATA[This blog post delves into the workings of the Document Automation Toolkit within AI Builder, detailing the steps involved in setting it up and the adjustments necessary to tailor it to specific needs.]]></summary></entry><entry><title type="html">A Collection of Must-Reads for the IT consultant</title><link href="https://dva81.github.io/blog/2024/Reading-list/" rel="alternate" type="text/html" title="A Collection of Must-Reads for the IT consultant"/><published>2024-02-29T00:00:00+00:00</published><updated>2024-02-29T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/Reading-list</id><content type="html" xml:base="https://dva81.github.io/blog/2024/Reading-list/"><![CDATA[<p>This is my collection of Must-Reads for the IT consultant! Happy Reading</p> <h2 id="the-devops-handbook-how-to-create-world-class-agility-reliability-and-security-in-technology-organizations-by-gene-kim">The DevOPS Handbook How to Create World-Class Agility, Reliability, and Security in Technology Organizations by Gene Kim</h2> <p>https://www.goodreads.com/book/show/26083308-the-devops-handbook</p> <h2 id="the-phoenix-project-by-gene-kim">The Phoenix Project by Gene Kim</h2> <p>https://www.goodreads.com/book/show/17255186-the-phoenix-project</p> <h2 id="pmi-acp-exam-prep-rapid-learning-to-pass-the-pmi-agile-certified-practitioner-pmi-acp-exam">Pmi-acp Exam Prep: Rapid Learning to Pass the Pmi Agile Certified Practitioner Pmi-acp Exam</h2> <p>https://www.goodreads.com/book/show/27276014-pmi-acp-exam-prep</p> <h2 id="the-scrum-guide">The Scrum Guide</h2> <p>https://www.scrum.org/resources/scrum-guide</p> <h2 id="scrum-product-ownership-by-robert-galen">Scrum Product ownership by Robert Galen</h2> <p>https://www.goodreads.com/book/show/44566037-scrum-product-ownership</p> <h2 id="atomic-design">Atomic Design</h2> <p>https://atomicdesign.bradfrost.com</p> <h2 id="flawless-consulting-a-guide-to-getting-your-expertise-used-by-peter-block">Flawless Consulting: A Guide to Getting Your Expertise Used by Peter Block</h2> <p>https://www.goodreads.com/book/show/8265721-flawless-consulting</p> <h2 id="the-7-habits-of-highly-effective-people-powerful-lessons-in-personal-change-by-steven-covey">The 7 Habits of Highly Effective People: Powerful Lessons in Personal Change by Steven Covey</h2> <p>https://www.goodreads.com/book/show/36072.The_7_Habits_of_Highly_Effective_People</p> <h2 id="about-the-archimate-modeling-language">About the ArchiMate Modeling Language</h2> <p>https://www.opengroup.org/archimate-forum/archimate-overview</p> <h2 id="dealing-with-people-you-cant-stand-how-to-bring-out-the-best-in-people-at-their-worst">Dealing with People You Can’t Stand: How to Bring Out the Best in People at Their Worst</h2> <p>https://www.goodreads.com/book/show/734384.Dealing_with_People_You_Can_t_Stand</p>]]></content><author><name></name></author><category term="Agile"/><category term="Leadership"/><summary type="html"><![CDATA[This is my collection of Must-Reads for the IT consultant.]]></summary></entry><entry><title type="html">Automating LinkedIn Posts - A Simple Scheduler for Busy Professionals</title><link href="https://dva81.github.io/blog/2024/Automating-LinkedIn-Posts/" rel="alternate" type="text/html" title="Automating LinkedIn Posts - A Simple Scheduler for Busy Professionals"/><published>2024-01-11T00:00:00+00:00</published><updated>2024-01-11T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/Automating-LinkedIn-Posts</id><content type="html" xml:base="https://dva81.github.io/blog/2024/Automating-LinkedIn-Posts/"><![CDATA[<p>As a practice lead, staying active on LinkedIn is crucial for promoting both yourself and your business. Many professionals dedicate time to post regularly or leverage scheduling functions to maintain a consistent online presence. However, when it comes to bulk scheduling posts over an extended period, LinkedIn lacks a native feature. Even the LinkedIn API falls short in this aspect, leaving you in need of a customized solution.</p> <p>In my quest to efficiently manage my LinkedIn content, I decided to create a simple scheduler that allows me to plan my posts for the next two years. In this blog post, I’ll walk you through the process of building a Power Automate Flow that meets this requirement.</p> <h1 id="requirements">Requirements</h1> <ul> <li>Dynamic Message Creation: Ensure variation in posts to keep your content engaging.</li> <li>Bi-weekly Scheduling: Automate the process to post every two weeks.</li> <li>Maintain Control: Craft a solution that keeps you in the driver’s seat.</li> </ul> <h1 id="building-the-power-automate-flow">Building the Power Automate Flow</h1> <p><img src="/assets/img/Automating LinkedIn Posts/Automating LinkedIn Posts (3).png" alt="Automating LinkedIn Posts"/></p> <h2 id="step-1-connect-with-linkedin">Step 1 Connect with LinkedIn</h2> <p>The first step involves creating a connection with LinkedIn using the standard free connector. Simply use your credentials to log in and establish the connection. <img src="/assets/img/Automating LinkedIn Posts/Automating LinkedIn Posts (2).png" alt="Automating LinkedIn Posts"/></p> <h2 id="step-2-dynamic-message-creation">Step 2 Dynamic Message Creation</h2> <p>To add variety to your posts, leverage Power Automate’s functions. Utilize the rand function to dynamically generate messages, ensuring a mix of content in your posts. <img src="/assets/img/Automating LinkedIn Posts/Automating LinkedIn Posts (5).png" alt="Automating LinkedIn Posts"/></p> <p><img src="/assets/img/Automating LinkedIn Posts/Automating LinkedIn Posts (1).png" alt="Automating LinkedIn Posts"/></p> <h2 id="step-3-bi-weekly-scheduling">Step 3 Bi-weekly Scheduling</h2> <p>Implement a scheduling mechanism within the flow to post every two weeks. This ensures a consistent presence on your LinkedIn profile. <img src="/assets/img/Automating LinkedIn Posts/Automating LinkedIn Posts (4).png" alt="Automating LinkedIn Posts"/></p> <p>Step 4 Stay in Control The entire solution is designed to keep you in control. By dynamically creating messages, scheduling bi-weekly posts, and leveraging Power Automate’s capabilities, you can confidently manage your LinkedIn content for the next two years.</p> <h1 id="access-the-power-automate-flow">Access the Power Automate Flow</h1> <p>To implement this solution, you can download the source files directly from <a href="https://github.com/dva81/PowerAutomate">GitHub</a>. Don’t forget to turn the flow on after setting it up.</p> <p>Empower your LinkedIn strategy by taking charge of your posting schedule with this simple yet effective Power Automate Flow. Stay visible, stay engaged, and let automation work for you!</p>]]></content><author><name></name></author><category term="PowerPlatform"/><summary type="html"><![CDATA[Many professionals dedicate time to post regularly or leverage scheduling functions to maintain a consistent online presence. However, when it comes to bulk scheduling posts, LinkedIn lacks a native feature, leaving you in need of a customized solution.]]></summary></entry><entry><title type="html">Mastering AI Model Validation - A Comprehensive Guide for Success</title><link href="https://dva81.github.io/blog/2024/Model-Validation/" rel="alternate" type="text/html" title="Mastering AI Model Validation - A Comprehensive Guide for Success"/><published>2024-01-02T00:00:00+00:00</published><updated>2024-01-02T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2024/Model-Validation</id><content type="html" xml:base="https://dva81.github.io/blog/2024/Model-Validation/"><![CDATA[<p>AI is everywhere these days! Yet is has existed since the beginning of the computer age. What is so different from 25 years ago? What makes it more powerful, more present? Why all the fuss? Over the last 10 years machine learning has become extremely powerful.</p> <p>Rather than programmers giving machine learning AIs a definitive list of instructions on how to complete a task, the AIs have to learn how to do the task themselves. There are many ways to attempt this, but the most popular approach involves software that is trained by example.</p> <h1 id="the-importance-of-validating-ai-models">The importance of validating AI models</h1> <p>AI is already involved in making important processes (like translations and summarizations) and yet AI is often biased, meaning its recommendations can be too. Time after time, researchers have found that <a href="https://www.newscientist.com/article/2166207-discriminating-algorithms-5-times-ai-showed-prejudice/">neural networks pick up biases from the data sets they are trained on</a>. For example, face recognition algorithms have much lower accuracy for anyone who is not a white man.</p> <p>AI decisions are also opaque or referred to as a “Black box”. How neural networks come to conclusions is very hard to analyze, meaning if they make a crucial mistake, such as missing cancer in an image, it’s very difficult to find out why they made the mistake or to hold them accountable.</p> <blockquote> <p>Validating AI models ensures accuracy, robustness, and reliability.</p> </blockquote> <p>Often creating and validating these sets in a time and resource consuming activity which does not always help the business case due to its labor-intensive nature. But validating AI models is necessary and ensures accuracy, robustness, and reliability.</p> <h1 id="data-splitting-for-model-validation">Data Splitting for Model Validation</h1> <p>The primary reason for data splitting is to avoid <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> , which occurs when the model is too complex and fits the training data too well, resulting in poor generalization performance on new data. <a href="https://machinelearningmastery.com/training-validation-test-split-and-cross-validation-done-right/">By splitting the data, we can ensure that the model is not overfitting to the training data and is generalizing well to new data</a>.</p> <p>Data splitting also provides an unbiased estimate of the model’s performance. If we use the same data for training and testing, the model will perform well on the training data but may not generalize well to new data. By using a separate testing set, we can obtain an unbiased estimate of the model’s performance on new data.</p> <h1 id="determining-validation-set-size">Determining Validation Set Size</h1> <p>To calculate the size of the validation set, there is no one-size-fits-all answer. However, a common approach is to use a 70/30 or 80/20 split for the training and validation sets, respectively. The size of the validation set should be large enough to provide a representative sample of the data but small enough to avoid overfitting. A good rule of thumb is to use at least 5% of the total data for the validation set. That means that for a population of 40000 you need a sample set of 2000.</p> <p>This online calculator helps to calculate sample sizes - <a href="https://www.calculator.net/sample-size-calculator.html">Sample Size Calculator</a> - for test and validation purposes. It provides insights into how the right validation set size contributes to an effective model assessment.</p> <p><img src="/assets/img/Model-validation/model-validation (4).png" alt="online calculator"/></p> <p>Methods of Model Validation: Cross Validation Cross-validation is a technique used in machine learning to evaluate the performance of a model. It involves dividing the dataset into smaller groups and then averaging the performance in each group to reduce the impact of partition randomness on the results. Nevertheless, each method has its own bias toward more positive or negative results. Here are some of the most commonly used cross-validation techniques:</p> <ol> <li> <p>Train-Test Split Method: This method randomly partitions the dataset into two subsets (called training and test sets) so that the predefined percentage of the entire dataset is in the training set. Then, we train our machine learning model on the training set and evaluate its performance on the test set. In this way, we are always sure that the samples used for training are not used for evaluation and vice versa.</p> </li> <li> <p>K-Fold Cross-Validation: In k-fold cross-validation, we first divide our dataset into k equally sized subsets. Then, we repeat the train-test method k times such that each time one of the k subsets is used as a test set and the rest k-1 subsets are used together as a training set. Finally, we compute the estimate of the model’s performance estimate by averaging the scores over the k trials.</p> </li> <li> <p>Leave-One-Out Cross-Validation: In leave-one-out cross-validation, we use all but one sample for training and the remaining sample for testing. We repeat this process for all samples in the dataset and average the performance over all trials.</p> </li> </ol> <h1 id="a-power-platform-example">A Power Platform Example</h1> <p>Let us say we have created a model in <a href="https://learn.microsoft.com/en-us/ai-builder/">AI builder</a>, and you want to see if it works as expected. Power Platform comes some standard prediction engines and features, still it is always a good idea to validate the outcome of a process.</p> <p>This technique can also be used with models created by other technologies as well.</p> <p>An easy way to do this in Power platform is with a Power Automate flow.</p> <ol> <li>Put your validation files in one folder. (you can even create them using a generation flow)</li> </ol> <p><img src="/assets/img/Model-validation/model-validation (3).png" alt="generation flow"/></p> <ol> <li> <p>Create a database with all the (meta)data you expect the model to find in the files.</p> </li> <li> <p>Create a flow that reads the files, runs the model and verifies the data found against the data source.</p> </li> <li> <p>Put the results in a file or dashboard.</p> </li> </ol> <p>It can look something like this:</p> <p><img src="/assets/img/Model-validation/model-validation (2).png" alt="flow"/></p> <h1 id="key-takeaways">Key Takeaways</h1> <p>Data splitting is essential for avoiding overfitting and obtaining an unbiased estimate of the model’s performance. It is a crucial step in the validation process. The choice of cross-validation technique depends on the size of the dataset, the number of features, and the computational resources available.</p> <p>These techniques are used to evaluate the performance of a machine learning model and to reduce the impact of partition randomness on the results. It helps ensure the overall accuracy, robustness, and reliability of the business process.</p> <p>Check out <a href="https://machinelearningmastery.com/">Machine Learning Mastery</a> where you’ll find “not so dry” material on this topic.</p>]]></content><author><name></name></author><category term="PowerPlatform"/><category term="AI"/><summary type="html"><![CDATA[AI is everywhere these days! Yet is has existed since the beginning of the computer age. What is so different from 25 years ago? What makes it more powerful, more present? Why all the fuss? Over the last 10 years machine learning has become extremely powerful.]]></summary></entry><entry><title type="html">Boosting Efficiency Docs-as-Code Strategies with Power Platform and Azure DevOps</title><link href="https://dva81.github.io/blog/2023/Docs-as-Code/" rel="alternate" type="text/html" title="Boosting Efficiency Docs-as-Code Strategies with Power Platform and Azure DevOps"/><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2023/Docs-as-Code</id><content type="html" xml:base="https://dva81.github.io/blog/2023/Docs-as-Code/"><![CDATA[<p>Some time ago I took over a Power Platform project right after first deploys to production. The application is a collection of over 30 Power Apps and more than 600 other components. With my DevOps background I was really happy to see the Azure DevOps setup the team had put in place to automate the deploy of the hundreds of Power Platform components as much a as possible.</p> <p>If you want to see how that can be done check out this video by Feline Parein. <a href="https://www.youtube.com/watch?v=pv8CyKrL5ds">Feline Parein - How to deploy Power Platform solution company-wide: Azure DevOps can help with CI/CD - YouTube</a></p> <p>As I was new to the project, I had no context to inform my understanding of what was going on or how things were connected. So, I started reading and looking for information. The developers were working on all the new features and releases, so they had little time to address my information needs.</p> <h1 id="why-docs-as-code">Why docs-as-code?</h1> <p>I found a maze of documents, snippets, designs and much more without any solid structure. Everything was there but it was in chaos. After talking to the team, I found out that, like most developers they don’t like to write documentation, switching tools and work environments, there was no review process, and they did not see the purpose of it as “everything we build is Low-Code”.</p> <blockquote> <p>My next mission was clear: improve the process, improve the documentation.</p> </blockquote> <p>With docs-as-code, you treat your documentation in the same way as your code. A quick start:</p> <ul> <li> <p>Format: pick a format that is easily versioned and transformed like .md. (Markdown)</p> </li> <li> <p>Versioning: Use a versioning system like GIT just as you would with code or configuration files. No need to append a date or v2.2x at the end.</p> </li> <li> <p>Automation: Transform the docs to any format needed (html, pdf, …) and distribute them in one go.</p> </li> <li> <p>Validation: Use an approval flow to get the documents approved and signed off.</p> </li> </ul> <h1 id="how-did-we-do-this">How did we do this?</h1> <p>The good thing was the team already tried to put documentation in repositories. We built on those first steps and completed the process.</p> <ul> <li>Create a wiki and publish it as code</li> </ul> <p><img src="/assets/img/Docs-as-code/Docs-as-Code1.png" alt="publish it as code"/></p> <ul> <li>.md as format. Markdown is a lightweight markup language that you can use to add formatting elements to plaintext text documents.</li> </ul> <p><img src="/assets/img/Docs-as-code/Docs-as-Code2.png" alt="publish it as code"/></p> <p><img src="/assets/img/Docs-as-code/Docs-as-Code3.png" alt="publish it as code"/></p> <ul> <li>In the pipeline step the output is generated in pdf format using a standard task in Azure DevOps. and attached to the artifact for deployment.</li> </ul> <p><img src="/assets/img/Docs-as-code/Docs-as-Code4.png" alt="publish it as code"/></p> <ul> <li>Releasing the documentation. In this step we automated the release to the right location for validation. After approval in Azure Devops that version was copied to the end user or production locations in the Power Platform solution.</li> </ul> <p><img src="/assets/img/Docs-as-code/Docs-as-Code6.png" alt="publish it as code"/></p> <p><img src="/assets/img/Docs-as-code/Docs-as-Code5.png" alt="publish it as code"/></p> <h1 id="bringing-it-all-together">Bringing it all together</h1> <blockquote> <p>“Happy cows make better milk.”</p> </blockquote> <p>By improving the overall documentation process, the team is now much more inclined to write documentation and maintain it. Connecting other people to the project is now much easier and we created better continuity for our client as well.</p> <hr/> <p>If you are more interested in the view of a technical writer, you might want to start your journey at http://writethedocs.org.</p> <hr/> <p>My focus is on structuring, automating and managing business processes using Agile and DevOps best practices. This creates team working environments where business continuity, transparency and human capital come first. Reach out to me on LinkedIn or check out my GitHub for more tips and tricks.</p>]]></content><author><name></name></author><category term="DevOps"/><category term="PowerPlatform"/><summary type="html"><![CDATA[Some time ago I took over a Power Platform project right after first deploys to production. The application is a collection of over 30 Power Apps and more than 600 other components. With my DevOps background I was really happy to see the Azure DevOps setup the team had put in place to automate the deploy of the hundreds of Power Platform components as much a as possible.]]></summary></entry><entry><title type="html">Agile product owner - key responsibilities in scrum development teams</title><link href="https://dva81.github.io/blog/2023/Agile-product-owner/" rel="alternate" type="text/html" title="Agile product owner - key responsibilities in scrum development teams"/><published>2023-11-12T00:00:00+00:00</published><updated>2023-11-12T00:00:00+00:00</updated><id>https://dva81.github.io/blog/2023/Agile-product-owner</id><content type="html" xml:base="https://dva81.github.io/blog/2023/Agile-product-owner/"><![CDATA[<p>Working at ACOLAD as an ECM Team Coach, most of my time is spent leading product teams in implementing common-of-the-shelf software (COTS) applications in enterprise environments. This includes introducing agile software development methodologies and connecting business users and developers in one t-shaped, multifunctional team.</p> <p>When looking at well performing scrum teams, one thing is clear: the product owner (PO) is the key to a successful product while the team is the locked treasure chest.</p> <h1 id="key-responsibilities-of-the-agile-product-owner">Key responsibilities of the Agile product owner</h1> <p>The PO plays one of the three roles (scrum master, dev team and product owner) of the scrum framework for agile project management. He s the only person responsible for managing the Product Backlog, put simply, the list of all things that needs to be done within the project. These can be either technical or user-centric, e.g. in the form of user stories (US). Clearly expressing product backlog items and translating stakeholder needs into usable instructions for creating and delivering solutions to those challenges. This is key to successfully starting any agile software development project.</p> <p>In many cases, business stakeholders on the client side are not ready to level with the tempo of the agile team and will not deliver information and requirements in a way the development team easily understands on what needs to be done. To connect the two, the PO must ensure that the Product Backlog is visible, transparent, clear to all and shows what the team will work on next to complete the goal or deadline!</p> <p>So how can an agile product owner gain trust from his team and value for the client in any software development project? As per my experience, I have listed the top 4 areas in which a Product Owner must perform well to keep scrum teams effective:</p> <h2 id="you-are-a-leader">You are a leader!</h2> <p>You lead the product development… getting the most value for the client. Agile dictates a self-organizing team and the product owner is second servant-leader in the team, next to the Agile lead or Scrum master. Always deliver added business value even if the development team wants to clean up technical debt. Get grip on your resources and lead them to a better product, but also be aware that code/infra is a living organism.</p> <h2 id="product-purpose---mission">Product Purpose - Mission</h2> <p>Know your product/mission in and out. Breathe it, live it. Envision it.</p> <p>Focus more on the team instead of business stakeholders. Stakeholders are important but ensure you don t lose the grip on the dev team!</p> <p>Take ownership of the project and demos, don t rely on the dev team to show what was accomplished as a team. Get involved in all the processes from deployment to production and also in maintenance operations, gathering necessary feedback to continuously improve.</p> <h2 id="know-your-backlog">Know your backlog</h2> <p>The team can only start working if they clearly know what to do. Backlog management must be done properly. The PO must challenge the development team to improve the backlog, connect with the target audience (users) and log progress in an issue tracker. </p> <p>What is important? Prioritize, not everything is priority 1.</p> <p>Incident control is not the same as the introduction of a new feature or US and vice versa, so they cannot be simply pulled in a sprint. Get your stories ready by making sure User Acceptance Tests (UAT) and operations acceptance criteria are clear.</p> <h2 id="delivering-quality">Delivering quality</h2> <p>Inspect and adapt! This is something we hear often in Agile. The PO is responsible for the value that will be delivered by the whole team, but value also means quality. There is no value if the users don t want to accept it and the product is not up to standards.</p> <p>The PO should check or inspect all aspects of Definition of Done. Are they realistic? Make sure that the project s priorities are well understood by the software development teams. In many cases the software development team only delivers what they think is important. Sure, communication is there, but people tend to work on what they like best and not on what should be done. The PO must keep a close eye on what will be delivered.</p> <h1 id="conclusion">Conclusion</h1> <p>The Agile product owner is the second servant leader in the scrum team, with focus on the word leader . Whether you re in an agile methodology or not, you know the role of the Product Owner is instrumental for the success of any software development project. In more unexperienced teams, where project management is evolving towards more agile practices and an agile mindset, the PO can make an even bigger difference in carrying the mission to deliver a high-quality solution.</p> <p>Updated 12/01/2022</p>]]></content><author><name></name></author><category term="Agile"/><category term="Leadership"/><summary type="html"><![CDATA[Working at ACOLAD as an ECM Team Coach, most of my time is spent leading product teams in implementing common-of-the-shelf software (COTS) applications in enterprise environments. This includes introducing agile software development methodologies and connecting business users and developers in one t-shaped, multifunctional team.]]></summary></entry></feed>